---
title: Tracing
---

## Overview ##

`judgeval`'s tracing module allows you to view your LLM application's execution from **end-to-end**. 

Using tracing, you can:
- Gain observability into **every layer of your agentic system**, from database queries to tool calling and text generation.
- Measure the performance of **each system component in any way** you want to measure it. For instance:
    - Catch regressions in **retrieval quality, factuality, answer relevance**, and 10+ other [**research-backed metrics**](/evaluation/scorers/introduction).
    - Quantify the **quality of each tool call** your agent makes
    - Track the latency of each system component
    - Count the token usage of each LLM generation
- Export your workflow runs to the Judgment platform for **real-time analysis** or as a dataset for [**offline experimentation**](/evaluation/introduction).


## Tracing Your Workflow ##

Setting up tracing with `judgeval` takes two simple steps:

### 1. Initialize a tracer with your API keys and project name

<CodeGroup>
```Python Python
from judgeval.tracer import Tracer

# loads from JUDGMENT_API_KEY and JUDGMENT_ORG_ID env vars
judgment = Tracer(project_name="my_project")
```
```Typescript Typescript
import { Tracer } from 'judgeval';

// The getInstance method ensures a singleton Tracer is used
const judgment = Tracer.getInstance({ projectName: "my_project" });
```
</CodeGroup>

<Note>
    The [Judgment tracer](/api_reference/trace) is a singleton object that should be shared across your application. 
    Your project name will be used to organize your traces in one place on the Judgment platform.
</Note>


### 2. Wrap your workflow components

`judgeval` provides wrapping mechanisms for your workflow components:

#### `wrap()` ####
The `wrap()` function goes over your LLM client (e.g. OpenAI, Anthropic, etc.) and captures metadata surrounding your LLM calls, such as:
- Latency
- Token usage
- Prompt/Completion
- Model name

Here's an example of using `wrap()` on an OpenAI client:
<CodeGroup>
```Python Python
from openai import OpenAI
from judgeval.tracer import wrap

client = wrap(OpenAI())
```
```Typescript Typescript
import OpenAI from 'openai';
import { wrap } from 'judgeval';

const client = wrap(new OpenAI());
```
</CodeGroup>

#### `@observe` (Python) / `observe()` (Typescript) ####
The `@observe` decorator (Python) or the `observe()` higher-order function (Typescript) wraps your functions/tools and captures metadata surrounding your function calls, such as:
- Latency
- Input/Output
- Span type (e.g. `retriever`, `tool`, `LLM call`, etc.)

With automatic deep tracing (enabled by default), you only need to observe top-level functions, and all nested function calls will be automatically traced. This significantly reduces the amount of instrumentation needed in your code.

Here's an example of using the observer mechanism:
<CodeGroup>
```Python Python
from judgeval.tracer import Tracer

judgment = Tracer(project_name="my_project")  # Deep tracing is enabled by default

# With automatic deep tracing, you only need to observe the top-level function
@judgment.observe(span_type="function")
def my_workflow():
    # All function calls inside will be automatically traced
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": "Hello!"}]
    )
    return response.choices[0].message.content

result = my_workflow()  # This will trace the entire workflow
```
```Typescript Typescript
import { Tracer, wrap } from 'judgeval';
import OpenAI from 'openai';

const judgment = Tracer.getInstance({ 
    projectName: "my_project"  # Deep tracing is enabled by default
});
const client = wrap(new OpenAI());

# With automatic deep tracing, you only need to observe the top-level function
const myWorkflow = judgment.observe({ spanType: "function" })(async () => {
    # All function calls inside will be automatically traced
    const response = await client.chat.completions.create({
        model: "gpt-4o",
        messages: [{ role: "user", content: "Hello!" }],
    });
    return response.choices[0]?.message?.content;
});

myWorkflow().then(console.log);  # This will trace the entire workflow
```
</CodeGroup>

<Note>
    `span_type` is a string that you can use to categorize and organize your trace spans. 
    Span types are displayed on the trace UI to easily navigate a visualization of your workflow. 
    Common span types include `tool`, `function`, `retriever`, `database`, `web search`, etc.
</Note>

#### Putting it all Together
Here's a complete example of using judgeval's tracing mechanisms:
<CodeGroup>
```Python Python
from judgeval.tracer import Tracer, wrap
from openai import OpenAI
from judgeval.scorers import AnswerRelevancyScorer

openai_client = wrap(OpenAI())
# loads from JUDGMENT_API_KEY and JUDGMENT_ORG_ID env vars
judgment = Tracer(project_name="my_project")

# Define a tool function that will be automatically traced when called from main
def my_tool():
    return "Hello world!"

# With automatic deep tracing, only the main function needs to be observed
@judgment.observe(span_type="function")
def main():
    # This tool call will be automatically traced thanks to deep tracing
    message = my_tool()
    res = openai_client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": message}]
    )
    return res.choices[0].message.content

# This implicitly starts a trace if one isn't active
# and saves it upon completion or error.
main_result = main()
```
```Typescript Typescript
import { Tracer, wrap } from 'judgeval';
import OpenAI from 'openai';

const judgment = Tracer.getInstance({ projectName: "my_project" });
const client = wrap(new OpenAI());

// Define a tool function that will be automatically traced when called from main
function myTool(): string {
    return "Hello world!";
}

// With automatic deep tracing, only the main function needs to be observed
const main = judgment.observe({ spanType: "function" })(async () => {
    // This tool call will be automatically traced thanks to deep tracing
    const message = myTool();
    const res = await client.chat.completions.create({
        model: "gpt-4o",
        messages: [{ role: "user", content: message }],
    });
    return res.choices[0]?.message?.content;
});

// Calling the observed function implicitly starts and saves the trace
async function runImplicitTrace() {
    try {
        const result = await main();
        console.log("Implicit trace completed:", result);
    } catch (error) {
        console.error("Implicit trace failed:", error);
        // Trace saved automatically by observe on completion/error
    }
}

runImplicitTrace();
```
</CodeGroup>

And the trace will appear on the Judgment platform as follows:

![Alt text](/images/basic_trace_example.png "Basic Trace Example")

### 3. Running Production Evaluations

Optionally, you can run asynchronous evaluations directly inside your traces.

This enables you to run evaluations on your **production data in real-time**, which can be useful for:
- **Guardrailing your production system** against quality regressions (hallucinations, toxic responses, revealing private data, etc.).
- Exporting production data for **offline experimentation** (e.g for A/B testing your workflow versions on relevant use cases).
- Getting **actionable insights** on how to fix common failure modes in your workflow (e.g. missing knowledge base info, suboptimal prompts, etc.). 

To execute an asynchronous evaluation, you can use the `trace.asyncEvaluate()` method (Typescript) or `judgment.async_evaluate()` (Python, assuming it operates on the currently active trace).

<CodeGroup>
```Python Python
from judgeval.tracer import Tracer
from judgeval.scorers import AnswerRelevancyScorer

judgment = Tracer(project_name="my_project")  # Deep tracing is enabled by default

# With automatic deep tracing, only the main function needs to be observed
@judgment.observe(name="agent_1", span_type="agent")
def main():
    query = "What is the capital of France?"
    # This function call will be automatically traced
    res = generate_response(query)
    
    # Assumes this runs within an active trace context implicitly started by @observe
    judgment.async_evaluate(
        scorers=[AnswerRelevancyScorer(threshold=1.0)],
        input=query,
        actual_output=res,
        model="gpt-4o",
    )
    return res

# This function will be automatically traced when called from main()
def generate_response(query):
    return "The capital of France is Paris."

main() # Call the observed function
```
```Typescript Typescript
import { Tracer, AnswerRelevancyScorer } from 'judgeval';

const judgment = Tracer.getInstance({ 
    projectName: "my_project"  # Deep tracing is enabled by default
});

# With automatic deep tracing, only the main function needs to be observed
const mainWithEval = judgment.observe({ spanType: "function" })(async () => {
    const query = "What is the capital of France?";
    # This function call will be automatically traced
    const res = generateResponse(query);

    const currentTrace = judgment.getCurrentTrace();
    if (currentTrace) {
        await currentTrace.asyncEvaluate(
            [new AnswerRelevancyScorer(1.0)], 
            {
                input: query,
                actualOutput: res,
                model: "gpt-4o"
            }
        );
    } else {
        console.warn("No active trace for async evaluation.");
    }
    return res;
});

# This function will be automatically traced when called from mainWithEval
function generateResponse(query) {
    return `The capital of France is Paris.`;
}

# Calling the observed function implicitly starts and saves the trace
async function runMainWithEvalTrace() {
    try {
        const result = await mainWithEval();
        console.log("Implicit trace completed:", result);
    } catch (error) {
        console.error("Implicit trace failed:", error);
        # Trace saved automatically by observe on completion/error
    }
}
```
</CodeGroup>

<Tip>
    Your async evaluations will be logged to the Judgment platform as part of the original trace and 
    a new evaluation will be created on the Judgment platform.
</Tip>

## Example: Music Recommendation Agent
In this video, we'll walk through all of the topics covered in this guide by tracing over a simple OpenAI API-based music recommendation agent.

<iframe
  width="560"
  height="315"
  src="https://www.youtube.com/embed/7g0fut06UxQ"
  title="YouTube video player"
  frameborder="0"
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
  allowfullscreen
></iframe>

## Multi-Agent Tracing with Automatic Deep Tracing

With automatic deep tracing, you can easily trace multi-agent systems without having to manually instrument every function. Here's an example:

<CodeGroup>
```Python Python
from judgeval.tracer import Tracer

judgment = Tracer(project_name="multi_agent_system")  # Deep tracing is enabled by default

# Only need to observe the top-level agent
@judgment.observe(name="agent_1", span_type="agent")
def agent_1(query):
    # This agent processes the query
    processed = process_query(query)
    # Then calls agent_2 for additional processing
    return agent_2(processed)

# No @observe needed - automatically traced when called from agent_1
def agent_2(data):
    # Second agent processes data from agent_1
    return generate_response(data)

# No @observe needed - automatically traced when called from agent_2
def generate_response(data):
    return f"Response for: {data}"

# No @observe needed - automatically traced when called from agent_1
def process_query(query):
    return f"Processed: {query}"

# The trace will show the full flow from agent_1 to agent_2
# including all internal function calls thanks to deep tracing
with judgment.trace(name="multi_agent_workflow") as trace:
    result = agent_1("What's the weather in San Francisco?")
    trace.save()
```
```Typescript Typescript
import { Tracer } from 'judgeval';

const judgment = Tracer.getInstance({ 
    projectName: "multi_agent_system"  # Deep tracing is enabled by default
});

# Only need to observe the top-level agent
const agent1 = judgment.observe({ 
    name: "agent_1", 
    spanType: "agent" 
})(async (query: string) => {
    # This agent processes the query
    const processed = processQuery(query);
    # Then calls agent_2 for additional processing
    return await agent2(processed);
});

# No observe needed - automatically traced when called from agent1
async function agent2(data: any) {
    # Second agent processes data from agent_1
    return generateResponse(data);
}

# No observe needed - automatically traced when called from agent2
function generateResponse(data: any) {
    return `Response for: ${data}`;
}

# No observe needed - automatically traced when called from agent1
function processQuery(query: string) {
    return `Processed: ${query}`;
}

# The trace will show the full flow from agent_1 to agent_2
async function runMultiAgentWorkflow() {
    const trace = judgment.startTrace("multi_agent_workflow");
    try {
        const result = await agent1("What's the weather in San Francisco?");
        console.log("Result:", result);
        await trace.save();
    } catch (error) {
        console.error("Multi-agent workflow failed:", error);
        await trace.save();
    }
}

runMultiAgentWorkflow();
```
</CodeGroup>

### Disabling Deep Tracing

Deep tracing is enabled by default, but you can disable it if you prefer manual control:

<CodeGroup>
```Python Python
# Disable deep tracing if you prefer manual control
judgment = Tracer(
    project_name="multi_agent_system",
    deep_tracing=False
)

# Now you'll need to explicitly observe each function
@judgment.observe(name="agent_1", span_type="agent")
def agent_1(query):
    processed = process_query(query)
    return agent_2(processed)
```
```Typescript Typescript
// Disable deep tracing if you prefer manual control
const judgment = Tracer.getInstance({ 
    projectName: "multi_agent_system",
    deepTracing: false
});

// Now you'll need to explicitly observe each function
const agent1 = judgment.observe({ 
    name: "agent_1", 
    spanType: "agent" 
})(async (query: string) => {
    const processed = processQuery(query);
    return await agent2(processed);
});
```
</CodeGroup>

## Advanced: Deep Tracing and Agent-to-Agent Calls ##

Judgment's tracing system includes powerful deep tracing capabilities that automatically capture function calls within your application without requiring explicit `@observe` decorators on every function.

### Automatic Deep Tracing

Deep tracing is enabled by default, allowing you to observe only top-level functions while all nested calls are automatically traced:

<CodeGroup>
```Python Python
from judgeval.tracer import Tracer

judgment = Tracer(project_name="multi_agent_system")  # Deep tracing is enabled by default

# Only need to observe the top-level agent
@judgment.observe(name="agent_1", span_type="agent")
def agent_1(query):
    # This agent processes the query
    processed = process_query(query)
    # Then calls agent_2 for additional processing
    return agent_2(processed)

# No @observe needed - automatically traced when called from agent_1
def agent_2(data):
    # Second agent processes data from agent_1
    return generate_response(data)

# No @observe needed - automatically traced when called from agent_2
def generate_response(data):
    return f"Response for: {data}"

# No @observe needed - automatically traced when called from agent_1
def process_query(query):
    return f"Processed: {query}"

# The trace will show the full flow from agent_1 to agent_2
# including all internal function calls thanks to deep tracing
with judgment.trace(name="multi_agent_workflow") as trace:
    result = agent_1("What's the weather in San Francisco?")
    trace.save()
```
```Typescript Typescript
import { Tracer } from 'judgeval';

const judgment = Tracer.getInstance({ 
    projectName: "multi_agent_system"  # Deep tracing is enabled by default
});

# Only need to observe the top-level agent
const agent1 = judgment.observe({ 
    name: "agent_1", 
    spanType: "agent" 
})(async (query: string) => {
    # This agent processes the query
    const processed = processQuery(query);
    # Then calls agent_2 for additional processing
    return await agent2(processed);
});

# No observe needed - automatically traced when called from agent1
async function agent2(data: any) {
    # Second agent processes data from agent_1
    return generateResponse(data);
}

# No observe needed - automatically traced when called from agent2
function generateResponse(data: any) {
    return `Response for: ${data}`;
}

# No observe needed - automatically traced when called from agent1
function processQuery(query: string) {
    return `Processed: ${query}`;
}

# The trace will show the full flow from agent_1 to agent_2
async function runMultiAgentWorkflow() {
    const trace = judgment.startTrace("multi_agent_workflow");
    try {
        const result = await agent1("What's the weather in San Francisco?");
        console.log(result);
        await trace.save();
    } catch (error) {
        console.error(error);
        await trace.save();
    }
}
```
</CodeGroup>

### Disabling Deep Tracing

If you prefer more control over what gets traced, you can disable deep tracing:

<CodeGroup>
```Python Python
# Disable deep tracing if you prefer manual control
judgment = Tracer(
    project_name="multi_agent_system",
    deep_tracing=False
)

# Now you'll need to explicitly observe each function
@judgment.observe(name="agent_1", span_type="agent")
def agent_1(query):
    processed = process_query(query)
    return agent_2(processed)
```
```Typescript Typescript
# Disable deep tracing if you prefer manual control
const judgment = Tracer.getInstance({ 
    projectName: "multi_agent_system",
    deepTracing: false
});

# Now you'll need to explicitly observe each function
const agent1 = judgment.observe({ 
    name: "agent_1", 
    spanType: "agent" 
})(async (query: string) => {
    const processed = processQuery(query);
    return await agent2(processed);
});

const agent2 = judgment.observe({ 
    name: "agent_2", 
    spanType: "agent" 
})(async (data: any) => {
    return await generateResponse(data);
});

const generateResponse = judgment.observe({ 
    spanType: "function" 
})(async (data: any) => {
    return `Response for: ${data}`;
});

const processQuery = judgment.observe({ 
    spanType: "function" 
})(async (query: string) => {
    return `Processed: ${query}`;
});
```
</CodeGroup>

This approach provides end-to-end visibility into complex multi-agent systems with minimal instrumentation, helping you understand how information flows between agents and identify bottlenecks or issues in your agent interactions.

## Advanced: Customizing Traces Using the Context Manager (Python) / Explicit Trace Client (Typescript) ##

In Python, if you need to customize your tracing context beyond the implicit behavior of `@observe`, you can use the `with judgment.trace()` context manager.
In Typescript, you achieve similar control by explicitly creating a `TraceClient` instance using `judgment.startTrace()` and manually calling methods like `save()` or `print()` on it.

The explicit trace client allows you to **save or print the state of the trace at any point in the workflow**.
This is useful for debugging or exporting any state of your workflow to run an evaluation from!

<Tip>
    Any functions wrapped with `judgment.observe()` called within the scope where the `TraceClient` is active will automatically be associated with that trace.
</Tip>

Here's an example of using explicit trace management:
<CodeGroup>
```Python Python
from judgeval.tracer import Tracer, wrap
from openai import OpenAI

judgment = Tracer(project_name="my_project")
client = wrap(OpenAI())

@judgment.observe(span_type="tool")
def my_tool():
    return "Hello world!"

def main():
    with judgment.trace(name="my_workflow") as trace:
        res = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": f"{my_tool()}"}]
        )
    
        trace.print()  # prints the state of the trace to console
        trace.save()  # saves the current state of the trace to the Judgment platform
        # Note: Python trace context likely saves automatically on exit

    return res.choices[0].message.content
```
```Typescript Typescript
import { Tracer, wrap } from 'judgeval';
import OpenAI from 'openai';

const judgment = Tracer.getInstance({ projectName: "my_project" });
const client = wrap(new OpenAI());

async function myTool(): Promise<string> {
    console.log("Hello world!");
    return "Hello world!";
}
const observedMyTool = judgment.observe({ spanType: "tool" })(myTool);

async function main() {
    # Start the trace explicitly
    const trace = judgment.startTrace("my_workflow"); 
    let resultMessage: string | null | undefined;

    try {
        const toolOutput = await observedMyTool(); # observedMyTool is associated with the active trace
        const res = await client.chat.completions.create({
            model: "gpt-4o",
            messages: [{ role: "user", content: toolOutput }],
        });
        resultMessage = res.choices[0]?.message?.content;

        trace.print(); # Prints the trace to console
        await trace.save(); # Saves the trace to the Judgment platform
        
    } catch (error) {
        console.error("Workflow failed:", error);
        await trace.save(); # Still save trace on error
    } finally {
        judgment.clearCurrentTrace(); # Clean up
    }

    return resultMessage;
}

main().then(console.log).catch(console.error);
```
</CodeGroup>

<Warning>
    In Python, the `with judgment.trace()` context manager should only be used if you need fine-grained control 
    over the trace lifecycle. In Typescript, explicit management via `startTrace()` and `trace.save()` is the standard way to gain this control.
    In most simple cases in Python, the `@observe` decorator is sufficient.
</Warning>

## Integrations ##

Judgment's tracing system integrates with various frameworks and libraries to provide seamless observability across your tech stack.
