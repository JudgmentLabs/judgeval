---
title: Integrating with LangGraph
---
We make it easy to integrate Judgeval with LangGraph. 
By simply adding the `JudgevalCallbackHandler` to your LangGraph workflow, you can trace your LangGraph workflow with Judgeval and also see insights from the execution.

<Note>
Make sure you have both <code>langgraph</code> and <code>langchain</code> installed in your environment. You can install them with:
<br/>
<code>pip install langgraph langchain</code>
</Note>

<Note>
We expect you to already be familiar with Judgment and its tools. If you are not, please refer to the [Getting Started](/getting_started) guide.
</Note>

## Judgeval Callback Handler

The Judgeval Callback Handler is a callback handler that can be used to trace your LangGraph workflow with Judgeval. We automatically trace all node visits, tool calls, and anything else you do in your workflow. We also store some additional fields on the handler that you can use to get more information about the execution:

- `executed_nodes`: list of node names that have been executed
- `executed_tools`: list of tool names that have been executed
- `executed_node_tools`: combined list of executions formatted as 
    - `[node_name, node_name, node_name:tool_name]` 

<Tip>
When using the JudgevalCallbackHandler, you do not have to add `@observe` decorators to your nodes/tools.
</Tip>

## Example Workflow

```python
from judgeval.common.tracer import Tracer, TraceManagerClient
from judgeval.integrations.langgraph import JudgevalCallbackHandler, set_global_handler
from langgraph.graph import StateGraph, END
from typing import TypedDict, Annotated, Sequence
from langchain_core.messages import HumanMessage, AIMessage
from langchain_openai import ChatOpenAI
import os

class State(TypedDict):
    messages: Sequence[HumanMessage | AIMessage]

PROJECT_NAME = "test-langgraph-project"

judgment = Tracer(
    api_key=os.getenv("JUDGMENT_API_KEY"), 
    project_name=PROJECT_NAME
)

llm = ChatOpenAI()

def process_message(state: State) -> State:
    messages = state["messages"]
    response = llm.invoke(messages)
    return {"messages": messages + [response]}

graph_builder = StateGraph(State)

graph_builder.add_node("process", process_message)
graph_builder.set_entry_point("process")

def finish_node(state: State) -> State:
    return state

graph_builder.add_node("finish_node", finish_node)
graph_builder.add_edge("process", "finish_node")
graph_builder.set_finish_point("finish_node")

graph = graph_builder.compile()

handler = JudgevalCallbackHandler(judgment)
set_global_handler(handler)  # This will automatically trace your entire workflow

result = graph.invoke({
    "messages": [HumanMessage(content="What is 5 + 5?")]
})

print(result)
# Accessing the JudgevalCallbackHandler attributes
print(handler.executed_nodes)
print(handler.executed_tools)
print(handler.executed_node_tools) # ['chatbot', 'tools', 'tools:search_tool']
```


View some of our demo code for more detailed examples.

- [Basic Workflow](https://github.com/JudgmentLabs/judgment-cookbook/blob/main/integrations/langgraph/basic.py)
- [Human in the Loop](https://github.com/JudgmentLabs/judgment-cookbook/blob/main/integrations/langgraph/human_in_the_loop/human_in_the_loop.py)
