---
title: Examples
---

## Overview
An `Example` is a basic unit of data in `judgeval` that allows you to run evaluation scorers on your LLM system. 
An `Example` is can be composed of a mixture of the following fields:
- `input`: [string]
- `actual_output`: [string, List[string]]
- `expected_output`: [string, List[String]]
- `retrieval_context`: [List[string]]
- `context`: [List[string]]
- `retrieval_context`: [List[string]]
- `additional_metadata`: [Dict[string, any]]
- `tools_called`: [List[string]]
- `expected_tools`: [List[string]]
- `name`: [String]
- `example_id`: [string]
- `example_index`: [int]
- `timestamp`: [string]
- `trace_id`: [string]

**Here's a sample of creating an `Example`:**

```python example_test.py
from judgeval.data import Example

example = Example(
    input="Who founded Microsoft?",
    actual_output="Bill Gates and Paul Allen.",
    expected_output="Bill Gates and Paul Allen founded Microsoft in New Mexico in 1975.",
    retrieval_context=["Bill Gates co-founded Microsoft with Paul Allen in 1975."],
    context=["Bill Gates and Paul Allen are the founders of Microsoft."],
)
```

## Example Fields 

Here, we cover the possible fields that make up an `Example`.

### Input 
The `input` field represents a sample interaction between a user and your LLM system. The input should represent the direct input to your prompt template(s), and **SHOULD NOT CONTAIN** your prompt template itself.

<Tip> 

Prompt templates are **hyperparameters** that you optimize for based on the scorer you're executing. 

Evaluation is always tied with optimization, so you should try to **isolate your system's independent variables** (e.g. prompt template, model, etc.) from your evaluation. 

</Tip>

### Actual Output 

The `actual_output` field represents what your LLM system outputs based on the `input`. 
This is the actual output of your LLM system created either at evaluation time or with saved answers.

```python actual_output.py
# Sample app implementation
import medical_chatbot

question = "Is sparkling water healthy?"
example = Example(
    input=question,
    actual_output=medical_chatbot.chat(question)
)
```

### Expected Output

The `expected_output` field is `Optional[str]` and represents the ideal output of your LLM system. 

One great part of `judgeval`'s scorers is that they use LLMs which have flexible evaluation criteria. You don't need to worry about your `expected_output` perfectly matching your `actual_output`!

To learn more about how `judgeval`'s scorers work, please see the [scorer docs](evaluation/scorers/introduction).

```python expected_output.py
# Sample app implementation
import medical_chatbot

question = "Is sparkling water healthy?"
example = Example(
    input=question,
    actual_output=medical_chatbot.chat(question),
    expected_output="Sparkling water is neither healthy nor unhealthy."
)
```

### Context 

The `context` field is `Optional[List[str]]` and represents information that is supplied to the LLM system as **ground truth**. 

For instance, context could be a list of facts that the LLM system is aware of. However, `context` should not be confused with `retrieval_context`.

<Tip>
In RAG systems, contextual information is retrieved from a vector database and is represented in `judgeval` by `retrieval_context`, 
not `context`. **If you're building a RAG system, you'll want to use `retrieval_context`.**
</Tip>

```python context.py
# Sample app implementation
import medical_chatbot

question = "Is sparkling water healthy?"
example = Example(
    input=question,
    actual_output=medical_chatbot.chat(question),
    expected_output="Sparkling water is neither healthy nor unhealthy.",
    context=["Sparkling water is a type of water that is carbonated."]
)
```

### Retrieval Context 

The `retrieval_context` field is `Optional[List[str]]` and represents the context that is actually retrieved from a vector database. 
This is often the context that is used to generate the `actual_output` in a RAG system.

<Tip>
Some common cases for using `retrieval_context` are:
- Checking for hallucinations in a RAG system
- Evaluating the quality of a retriever model by comparing `retrieval_context` to `context`
</Tip>

```python retrieval_context.py
# Sample app implementation
import medical_chatbot

question = "Is sparkling water healthy?"
example = Example(
    input=question,
    actual_output=medical_chatbot.chat(question),
    expected_output="Sparkling water is neither healthy nor unhealthy.",
    context=["Sparkling water is a type of water that is carbonated."],
    retrieval_context=["Sparkling water is carbonated and has no calories."]
)
```

### Additional Metadata

TODO: find someone more knowledgeable to write this section

### Tools Called

The `tools_called` field is supplied as `Optional[List[str]]` and lists the names of tools used by the LLM while answering a query.

### Expected Tools

`expected_tools: Optional[List[str]]` provides a list of tools that we expect the LLM to use when answering a query. Some pseudocode of what this could look like using LangChain is:

```python news_agent.py

...
@judgment.observe(span_type="tool")
async def search_tavily(query):
    # code for LLM tooling below
...

llm = init_chat_model("gpt-4o-mini", model_provider="openai")
llm_with_tools = llm.bind_tools([search_tavily])

input = "Please search for the news headlines for today."
response = llm_with_tools.invoke(input)
actual_output = response.content
actual_tools_called = [tool.name for tool in response.tool_calls]

example = Example(
    input=input,
    actual_output=llm_with_tools.invoke(input),
    tools_called=actual_tools_called,
    expected_tools=['search_tavily']
)

```

### Name 

TODO: The name of the example?

### Example ID

### Example Index 

### Timestamp  

### Trace ID  

## Conclusion 

Congratulations! ðŸŽ‰

You've learned how to create an `Example` and can begin using them to execute evaluations or create [datasets](/evaluation/data_datasets).
