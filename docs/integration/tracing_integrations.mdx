---
title: 'Integrations with Observability Platforms'
description: 'How to use Judgment with Arize, Braintrust, Langfuse, Langsmith, and other LLM observability/tracing platforms.'
---

# Integrating Judgment with LLM Observability Platforms

You can leverage Judgment's powerful evaluation capabilities directly within popular LLM observability and tracing platforms like Arize, Braintrust, Langfuse, and Langsmith. This allows you to incorporate custom, sophisticated evaluation logic (like checking answer relevancy, tone, or custom metrics) into your existing tracing and experiment tracking workflows.

The core idea is to create a wrapper function within your code that acts as a custom "evaluator" or "scorer" for the specific platform. This function will:

1.  Receive data (like input prompts, model outputs, reference outputs) from the platform's evaluation or tracing context.
2.  Extract and format this data into a `judgeval.data.Example` object.
3.  Instantiate the desired `judgeval.scorer` (e.g., `AnswerRelevancyScorer`).
4.  Initialize the `judgeval.JudgmentClient`.
5.  Run the evaluation using `judgment_client.run_evaluation(...)`.
6.  Extract the relevant results (score, label, explanation) from the `judgeval` output.
7.  Format and return these results in the specific structure expected by the observability platform.

## Prerequisites

Ensure you have the necessary libraries installed and imported:

```python
# Judgment imports
from judgeval import JudgmentClient
from judgeval.data import Example
from judgeval.scorers import AnswerRelevancyScorer # Or your specific scorer

# Other common imports (add platform-specific ones within each tab)
import json
import os 
```

You will also need to initialize the `JudgmentClient`.

```python
# Initialize Judgment Client (Universal)
judgment_client = JudgmentClient()

# Platform-specific client initializations moved to respective tabs below
```

## Platform-Specific Integration Examples

Below are examples demonstrating how to create and integrate a Judgeval scorer for each platform, based on the cookbook examples using `AnswerRelevancyScorer`.

<Tabs>
  <Tab title="Arize">
    ### Arize

    **Platform:** Arize

    **Terminology:** Evaluator

    **Integration Point:** `client.run_experiment(..., evaluators=[your_function], ...)`

    Arize's `run_experiment` expects evaluators to return an `EvaluationResult` object containing `score`, `label`, and `explanation`.

    ```python
    # Platform-specific imports
    from arize.experimental.datasets import ArizeDatasetsClient
    from arize.experimental.datasets.experiments.types import EvaluationResult
    import json

    # Initialize Arize Client (Example)
    arize_client = ArizeDatasetsClient(
        api_key=os.environ["ARIZE_API_KEY"], 
        space_key=os.environ["ARIZE_SPACE_KEY"]
    )

    # Evaluator function signature expected by Arize
    def use_judgment_scorer(input, output, experiment_output):
        print(f"input: {input}  \\noutput: {output}\\nexperiment_output: {experiment_output}")

        # 1. Extract data from Arize context
        # Input is often a JSON string, outputs are dicts with nested JSON strings
        input_data = json.loads(input)
        input_str = input_data['messages'][0]['content']

        actual_output_messages = json.loads(output['attributes.llm.output_messages'])
        actual_output_str = actual_output_messages[0]['message.content']

        expected_output_messages = json.loads(experiment_output['attributes.llm.output_messages'])
        expected_output_str = expected_output_messages[0]['message.content']

        # 2. Create Judgeval Example
        example = Example(
            input=input_str,
            actual_output=actual_output_str,
            expected_output=expected_output_str
        )

        # 3. Instantiate Scorer
        scorer = AnswerRelevancyScorer(threshold=0.5) # Example scorer

        # 4. Run Judgeval Evaluation (Assuming judgment_client is initialized)
        results = judgment_client.run_evaluation(
            project_name="Arize-Cookbook-Eval", # Choose your project name
            eval_run_name="Answer Relevancy - Arize", # Choose run name
            examples=[example],
            scorers=[scorer],
            override=True,
            model="gpt-4o", # Or your preferred model
        )

        # 5. Extract results
        scorers_data = results[0].scorers_data[0]
        score = scorers_data.score
        success = scorers_data.success
        label = "Pass" if success else "Fail"
        explanation = scorers_data.reason

        # 6. Format and return as EvaluationResult
        return EvaluationResult(score=score, label=label, explanation=explanation)

    # Example Usage:
    client.run_experiment(
        space_id="YOUR_SPACE_ID",
        dataset_name="YOUR_DATASET_NAME",
        task=your_task_function, # Your function that generates model output
        evaluators=[use_judgment_scorer],
        experiment_name="my-judgeval-experiment",
    )
    ```
  </Tab>
  <Tab title="Braintrust">
    ### Braintrust

    **Platform:** Braintrust

    **Terminology:** Score Function

    **Integration Point:** `Eval(..., scores=[your_function], ...)`

    Braintrust's `Eval` expects score functions to return a single numeric score value.

    ```python
    # Platform-specific imports
    from braintrust import Eval
    import os

    # Initialize Braintrust Client (implicitly done via Eval or other setup)
    # Ensure BRAINTRUST_API_KEY environment variable is set
    # No explicit client object needed for this example, 
    # but ensure API key is in environment.

    # Score function signature expected by Braintrust
    def use_judgment_scorer(input, expected, output):
        print(f"input: {input}, expected: {expected}, output: {output}")

        # 1. Data is directly available as arguments

        # 2. Create Judgeval Example
        example = Example(
            input=input,
            expected_output=expected,
            actual_output=output
            )

        # 3. Instantiate Scorer
        scorer = AnswerRelevancyScorer(threshold=0.5) # Example scorer

        # 4. Run Judgeval Evaluation (Assuming judgment_client is initialized)
        results = judgment_client.run_evaluation(
            project_name="Braintrust-Cookbook-Eval", # Choose your project name
            eval_run_name="Answer Relevancy - Braintrust", # Choose run name
            examples=[example],
            scorers=[scorer],
            override=True,
            model="gpt-4o", # Or your preferred model
        )

        # 5. Extract results
        score = results[0].scorers_data[0].score

        # 6. Format and return the numeric score
        return score

    # Example Usage:
    Eval(
        project="Your Braintrust Project",
        experiment="My Judgeval Experiment",
        data=your_data_source, # Function or list providing input/expected pairs
        task=your_task_function, # Your function that generates model output
        scores=[use_judgment_scorer],
    )
    ```
   </Tab>
   <Tab title="Langfuse">
    ### Langfuse

    **Platform:** Langfuse

    **Terminology:** Score

    **Integration Point:** `langfuse.score(trace_id=..., name=..., value=your_numeric_score)`

    Langfuse allows attaching scores to existing traces. You typically loop through traces and call `langfuse.score` for each, providing a numeric value.

    ```python
    # Platform-specific imports
    from langfuse import Langfuse
    import os

    # Initialize Langfuse Client (Example)
    langfuse = Langfuse(
        secret_key=os.environ["LANGFUSE_SECRET_KEY"],
        public_key=os.environ["LANGFUSE_PUBLIC_KEY"],
        host="https://us.cloud.langfuse.com" # Or your self-hosted instance
    )

    # Helper function to calculate score (can be defined separately or inline)
    def calculate_judgeval_score(input_str, actual_output_str, expected_output_str=None):
        print(f"input: {input_str}\\n expected: {expected_output_str}\\n output: {actual_output_str}")

        # 2. Create Judgeval Example
        example = Example(
            input=input_str,
            expected_output=expected_output_str, # Can be None if not available
            actual_output=actual_output_str
        )

        # 3. Instantiate Scorer
        scorer = AnswerRelevancyScorer(threshold=0.5) # Example scorer

        # 4. Run Judgeval Evaluation (Assuming judgment_client is initialized)
        results = judgment_client.run_evaluation(
            project_name="Langfuse-Cookbook-Eval", # Choose your project name
            eval_run_name="Answer Relevancy - Langfuse", # Choose run name
            examples=[example],
            scorers=[scorer],
            override=True,
            model="gpt-4o", # Or your preferred model
        )

        # 5. Extract results
        score = results[0].scorers_data[0].score if results and results[0].scorers_data else 0.0 # Handle potential errors

        # 6. Return numeric score
        return score

    # Example Usage (assuming 'traces_batch' contains fetched Langfuse traces):
    for trace in traces_batch:
        try:
            # 1. Extract data from Langfuse trace context
            # Adjust extraction based on your trace structure
            input_arg = trace.input.get('args', [None])[0] or trace.input.get('input', None)
            output_val = trace.output

            if input_arg is None or output_val is None:
                print(f"Skipping trace {trace.id}: Missing input or output.")
                continue

            # Expected output might not always be present in a trace
            expected_val = None # Or extract if available, e.g., trace.expected

            # Calculate score using the helper function
            judgeval_score = calculate_judgeval_score(
                input_str=str(input_arg), # Ensure strings
                actual_output_str=str(output_val),
                expected_output_str=str(expected_val) if expected_val else None
            )

            # Attach score to the Langfuse trace
            langfuse.score(
                trace_id=trace.id,
                name="judgeval_answer_relevancy", # Choose a descriptive name
                value=judgeval_score # Pass the numeric score
            )
            print(f"Scored trace {trace.id} with {judgeval_score}")
        except Exception as e:
            print(f"Error processing trace {trace.id}: {e}")

    ```
   </Tab>
   <Tab title="Langsmith">
    ### LangSmith

    **Platform:** LangSmith

    **Terminology:** Custom Evaluator

    **Integration Point:** `client.evaluate(..., evaluators=[your_function], ...)`

    LangSmith evaluators typically receive `run` and `example` objects and should return a dictionary containing a `key` (scorer name) and a numeric `score`.

    ```python
    # Platform-specific imports
    from langsmith import Client, wrappers
    from openai import OpenAI # Example dependency
    import os

    # Initialize LangSmith Client (Example)
    langsmith_client = Client()
    # Example of wrapping another client if needed by your task function
    openai_client = wrappers.wrap_openai(OpenAI())

    # Evaluator function signature expected by LangSmith
    def use_judgment_scorer(run, example):
        # 1. Extract data from LangSmith context
        input_str = example.inputs.get("input")
        # Handle potential variations in output key ('response' or 'output')
        output_str = run.outputs.get("response") or run.outputs.get("output")
        # Expected output might be under 'expected' or other keys
        expected_str = example.outputs.get("expected") if example.outputs else None

        if input_str is None or output_str is None:
            print("Warning: Skipping evaluation due to missing input or output.")
            # Return a default score or handle as needed
            return {"key": "judgment_answer_relevancy", "score": 0.0, "comment": "Missing input/output"}

        # 2. Create Judgeval Example
        judgeval_example = Example(
            input=input_str,
            expected_output=expected_str, # Can be None
            actual_output=output_str
            )

        # 3. Instantiate Scorer
        scorer = AnswerRelevancyScorer(threshold=0.5) # Example scorer

        # 4. Run Judgeval Evaluation (Assuming judgment_client is initialized)
        try:
            results = judgment_client.run_evaluation(
                project_name="LangSmith-Cookbook-Eval", # Choose your project name
                eval_run_name="Answer Relevancy - LangSmith", # Choose run name
                examples=[judgeval_example],
                scorers=[scorer],
                override=True,
                model="gpt-4o", # Or your preferred model
            )
            # 5. Extract results
            score = results[0].scorers_data[0].score if results and results[0].scorers_data else None
        except Exception as e:
            print(f"Error during Judgeval evaluation: {e}")
            score = None

        if score is None:
            print("Warning: Could not extract score from Judgment evaluation results.")
            # Return a default score or raise an error, depending on desired behavior
            return {"key": "judgment_answer_relevancy", "score": 0.0, "comment": "Evaluation failed"}

        # 6. Format and return dictionary with key and score
        return {"key": "judgment_answer_relevancy", "score": score}

    # Example Usage:
    experiment_results = client.evaluate(
        your_target_function, # Your function generating model output
        data = "your_langsmith_dataset_name", # Dataset name or ID
        evaluators = [
            use_judgment_scorer
        ],
        experiment_prefix = "my-judgeval-experiment",
    )

    ```
   </Tab>
</Tabs>

## Summary

By creating these simple wrapper functions, you can seamlessly integrate Judgeval's evaluations into your preferred LLM observability platform. Remember to adapt the data extraction and return value formatting steps based on the specific requirements of each platform and the structure of your own data and traces. 