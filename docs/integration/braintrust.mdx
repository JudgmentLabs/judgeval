---
title: Integrating with Braintrust
---
We make it easy to integrate Judgeval with Braintrust. 
By using our custom scorer integration, you can leverage Judgment's evaluation capabilities within your Braintrust experiments and benchmarks.

<Note>
We expect you to already be familiar with Judgment and its tools. If you are not, please refer to the [Getting Started](/getting_started) guide.
</Note>

## Braintrust Custom Scorer

Braintrust's custom scorer provides a flexible way to incorporate Judgeval's advanced evaluation capabilities into your experiments. By calling Judgeval scorers within Braintrust's custom scorer, you can leverage our specialized scorers and detailed analysis within your existing Braintrust workflow.

<Tip>
For more information on the capabilities of Judgeval's scorers and the metrics we offer, please refer to [Scorers](/evaluation/scorers/introduction) page.
</Tip>
## Example Usage

```python
from braintrust import Eval
from judgeval import JudgmentClient
from judgeval.data import Example
from judgeval.scorers import AnswerRelevancyScorer

# Initialize the Judgment client
judgment_client = JudgmentClient()

def use_judgment_scorer(input, expected, output):
    # Create an Example object with input, expected output, and actual output
    example = Example(
        input=input,
        expected_output=expected,
        actual_output=output
    )

    # Initialize the scorer with desired parameters
    scorer = AnswerRelevancyScorer(threshold=0.5)
    
    # Run the evaluation
    results = judgment_client.run_evaluation(
        project_name="Your Project Name",
        eval_run_name="Evaluation Run Name",
        examples=[example],
        scorers=[scorer],
        override=True,
        model="gpt-4",  # or your preferred model
    )
    
    # Return the score from the first example and scorer
    return results[0].scorers_data[0].score

# Use the scorer in your Braintrust evaluation
Eval(
    "Your Project Name",
    data=lambda: [
        {
            "input": "Your input here",
            "expected": "Expected output here",
            "actual_output": "Model's output here"
        }
    ],
    task=lambda input: "Your model's response function here",
    scores=[use_judgment_scorer],
)
```

View some of our demo code for a more detailed example.

- [Travel Itinerary Example](https://github.com/JudgmentLabs/judgment-cookbook/blob/main/integrations/braintrust/braintrust_cookbook.py)
