# generated by datamodel-codegen:
#   filename:  openapi_new.json
#   timestamp: 2025-06-22T20:02:28+00:00

from __future__ import annotations

from typing import Any, Dict, List, Optional, Union

from pydantic import BaseModel, Field


class EvalResultsFetchByProjectSortedLimitJudgmentType(BaseModel):
    project_name: str = Field(..., title='Project Name')
    limit: int = Field(..., title='Limit')


class DatasetDeleteExamplesJudgmentType(BaseModel):
    dataset_alias: str = Field(..., title='Dataset Alias')
    example_ids: List[str] = Field(..., title='Example Ids')


class DatasetFetchJudgmentType(BaseModel):
    dataset_alias: str = Field(..., title='Dataset Alias')
    project_name: str = Field(..., title='Project Name')


class DatasetFetchByProjectJudgmentType(BaseModel):
    project_name: str = Field(..., title='Project Name')


class DatasetDeleteJudgmentType(BaseModel):
    dataset_alias: str = Field(..., title='Dataset Alias')
    project_name: str = Field(..., title='Project Name')


class TraceFetchJudgmentType(BaseModel):
    trace_id: str = Field(..., title='Trace Id')


class BatchTraceFetchJudgmentType(BaseModel):
    trace_ids: List[str] = Field(..., title='Trace Ids')


class TracesFetchByProjectJudgmentType(BaseModel):
    project_name: str = Field(..., title='Project Name')


class TraceDeleteBatchJudgmentType(BaseModel):
    trace_ids: List[str] = Field(..., title='Trace Ids')


class FetchClassifierScorersRequestJudgmentType(BaseModel):
    user_filter: Optional[bool] = Field(False, title='User Filter')


class ScorerJudgmentType(BaseModel):
    threshold: float = Field(..., title='Threshold')
    score_type: str = Field(..., title='Score Type')
    kwargs: Optional[Dict[str, Any]] = Field(None, title='Kwargs')


class ValidationErrorJudgmentType(BaseModel):
    loc: List[Union[str, int]] = Field(..., title='Location')
    msg: str = Field(..., title='Message')
    type: str = Field(..., title='Error Type')


class MessageItemJudgmentType(BaseModel):
    role: str = Field(..., title='Role')
    content: str = Field(..., title='Content')


class ToolJudgmentType(BaseModel):
    tool_name: str = Field(..., title='Tool Name')
    parameters: Optional[Dict[str, Any]] = Field(None, title='Parameters')
    agent_name: Optional[str] = Field(None, title='Agent Name')
    result_dependencies: Optional[List[Dict[str, Any]]] = Field(
        None, title='Result Dependencies'
    )
    action_dependencies: Optional[List[Dict[str, Any]]] = Field(
        None, title='Action Dependencies'
    )
    require_all: Optional[bool] = Field(None, title='Require All')


class ScorerDataJudgmentType(BaseModel):
    name: str = Field(..., title='Name')
    threshold: float = Field(..., title='Threshold')
    success: bool = Field(..., title='Success')
    score: Optional[float] = Field(None, title='Score')
    reason: Optional[str] = Field(None, title='Reason')
    strict_mode: Optional[bool] = Field(None, title='Strict Mode')
    evaluation_model: Optional[Union[List[str], str]] = Field(
        None, title='Evaluation Model'
    )
    error: Optional[str] = Field(None, title='Error')
    evaluation_cost: Optional[float] = Field(None, title='Evaluation Cost')
    verbose_logs: Optional[str] = Field(None, title='Verbose Logs')
    additional_metadata: Optional[Dict[str, Any]] = Field(
        None, title='Additional Metadata'
    )


class CustomExampleJudgmentType(BaseModel):
    input: Optional[Dict[str, Any]] = Field(None, title='Input')
    actual_output: Optional[Dict[str, Any]] = Field(None, title='Actual Output')
    expected_output: Optional[Dict[str, Any]] = Field(None, title='Expected Output')
    context: Optional[List[str]] = Field(None, title='Context')
    retrieval_context: Optional[List[str]] = Field(None, title='Retrieval Context')
    additional_metadata: Optional[Dict[str, Any]] = Field(
        None, title='Additional Metadata'
    )
    tools_called: Optional[List[str]] = Field(None, title='Tools Called')
    expected_tools: Optional[List[str]] = Field(None, title='Expected Tools')
    name: Optional[str] = Field(None, title='Name')
    example_id: Optional[str] = Field(None, title='Example Id')
    example_index: Optional[int] = Field(None, title='Example Index')
    timestamp: Optional[str] = Field(None, title='Timestamp')
    trace_id: Optional[str] = Field(None, title='Trace Id')


class TraceUsageJudgmentType(BaseModel):
    prompt_tokens: Optional[int] = Field(None, title='Prompt Tokens')
    completion_tokens: Optional[int] = Field(None, title='Completion Tokens')
    total_tokens: Optional[int] = Field(None, title='Total Tokens')
    prompt_tokens_cost_usd: Optional[float] = Field(
        None, title='Prompt Tokens Cost Usd'
    )
    completion_tokens_cost_usd: Optional[float] = Field(
        None, title='Completion Tokens Cost Usd'
    )
    total_cost_usd: Optional[float] = Field(None, title='Total Cost Usd')
    model_name: Optional[str] = Field(None, title='Model Name')


class HTTPValidationErrorJudgmentType(BaseModel):
    detail: Optional[List[ValidationErrorJudgmentType]] = Field(None, title='Detail')


class ClassifierScorerRequestJudgmentType(BaseModel):
    name: str = Field(..., title='Name')
    conversation: List[MessageItemJudgmentType] = Field(
        ..., min_items=1, title='Conversation'
    )
    options: Dict[str, float] = Field(..., title='Options')
    slug: Optional[str] = Field(None, title='Slug')


class ExampleJudgmentType(BaseModel):
    input: Optional[Union[str, Dict[str, Any]]] = Field(None, title='Input')
    actual_output: Optional[Union[str, List[str]]] = Field(None, title='Actual Output')
    expected_output: Optional[Union[str, List[str]]] = Field(
        None, title='Expected Output'
    )
    context: Optional[List[str]] = Field(None, title='Context')
    retrieval_context: Optional[List[str]] = Field(None, title='Retrieval Context')
    additional_metadata: Optional[Dict[str, Any]] = Field(
        None, title='Additional Metadata'
    )
    tools_called: Optional[List[str]] = Field(None, title='Tools Called')
    expected_tools: Optional[List[ToolJudgmentType]] = Field(
        None, title='Expected Tools'
    )
    name: Optional[str] = Field(None, title='Name')
    example_id: str = Field(..., title='Example Id')
    example_index: Optional[int] = Field(None, title='Example Index')
    created_at: Optional[str] = Field(None, title='Created At')
    trace_id: Optional[str] = Field(None, title='Trace Id')
    trace_span_id: Optional[str] = Field(None, title='Trace Span Id')
    dataset_id: Optional[str] = Field(None, title='Dataset Id')


class TraceSpanJudgmentType(BaseModel):
    span_id: str = Field(..., title='Span Id')
    trace_id: str = Field(..., title='Trace Id')
    function: str = Field(..., title='Function')
    depth: int = Field(..., title='Depth')
    created_at: Any = Field(None, title='Created At')
    parent_span_id: Optional[str] = Field(None, title='Parent Span Id')
    span_type: Optional[str] = Field('span', title='Span Type')
    inputs: Optional[Dict[str, Any]] = Field(None, title='Inputs')
    error: Optional[Dict[str, Any]] = Field(None, title='Error')
    output: Any = Field(None, title='Output')
    usage: Optional[TraceUsageJudgmentType] = None
    duration: Optional[float] = Field(None, title='Duration')
    annotation: Optional[List[Dict[str, Any]]] = Field(None, title='Annotation')
    expected_tools: Optional[List[ToolJudgmentType]] = Field(
        None, title='Expected Tools'
    )
    additional_metadata: Optional[Dict[str, Any]] = Field(
        None, title='Additional Metadata'
    )
    has_evaluation: Optional[bool] = Field(False, title='Has Evaluation')
    agent_name: Optional[str] = Field(None, title='Agent Name')
    state_before: Optional[Dict[str, Any]] = Field(None, title='State Before')
    state_after: Optional[Dict[str, Any]] = Field(None, title='State After')


class JudgmentEvalJudgmentType(BaseModel):
    project_name: Optional[str] = Field(None, title='Project Name')
    eval_name: Optional[str] = Field(None, title='Eval Name')
    examples: List[ExampleJudgmentType] = Field(..., title='Examples')
    scorers: List[ScorerJudgmentType] = Field(..., title='Scorers')
    model: Union[str, List[str]] = Field(..., title='Model')
    aggregator: Optional[str] = Field(None, title='Aggregator')
    metadata: Optional[Dict[str, Any]] = Field({}, title='Metadata')
    judgment_api_key: Optional[str] = Field(None, title='Judgment Api Key')
    log_results: bool = Field(..., title='Log Results')
    append: Optional[bool] = Field(False, title='Append')
    override_existing_eval_run_name: Optional[bool] = Field(
        False, title='Override Existing Eval Run Name'
    )
    rules: Optional[List] = Field(None, title='Rules')
    trace_span_id: Optional[str] = Field(None, title='Trace Span Id')


class DatasetInsertExamplesJudgmentType(BaseModel):
    dataset_alias: str = Field(..., title='Dataset Alias')
    examples: List[ExampleJudgmentType] = Field(..., title='Examples')
    project_name: str = Field(..., title='Project Name')


class TraceSaveJudgmentType(BaseModel):
    trace_id: str = Field(..., title='Trace Id')
    name: str = Field(..., title='Name')
    created_at: str = Field(..., title='Created At')
    duration: float = Field(..., title='Duration')
    trace_spans: List[TraceSpanJudgmentType] = Field(..., title='Trace Spans')
    overwrite: Optional[bool] = Field(False, title='Overwrite')
    offline_mode: Optional[bool] = Field(False, title='Offline Mode')
    rules: Optional[Dict[str, Any]] = Field(None, title='Rules')
    has_notification: Optional[bool] = Field(False, title='Has Notification')
    customer_id: Optional[str] = Field(None, title='Customer Id')
    tags: Optional[List[str]] = Field(None, title='Tags')
    project_name: str = Field(..., title='Project Name')
    evaluation_runs: List[JudgmentEvalJudgmentType] = Field(
        ..., title='Evaluation Runs'
    )


class TraceJudgmentType(BaseModel):
    trace_id: str = Field(..., title='Trace Id')
    name: str = Field(..., title='Name')
    created_at: str = Field(..., title='Created At')
    duration: float = Field(..., title='Duration')
    trace_spans: List[TraceSpanJudgmentType] = Field(..., title='Trace Spans')
    overwrite: Optional[bool] = Field(False, title='Overwrite')
    offline_mode: Optional[bool] = Field(False, title='Offline Mode')
    rules: Optional[Dict[str, Any]] = Field(None, title='Rules')
    has_notification: Optional[bool] = Field(False, title='Has Notification')
    customer_id: Optional[str] = Field(None, title='Customer Id')
    tags: Optional[List[str]] = Field(None, title='Tags')


class ScoringResultJudgmentType(BaseModel):
    success: bool = Field(..., title='Success')
    scorers_data: Optional[List[ScorerDataJudgmentType]] = Field(
        ..., title='Scorers Data'
    )
    name: Optional[str] = Field(None, title='Name')
    data_object: Optional[
        Union[TraceSpanJudgmentType, CustomExampleJudgmentType, ExampleJudgmentType]
    ] = Field(None, title='Data Object')
    trace_id: Optional[str] = Field(None, title='Trace Id')
    run_duration: Optional[float] = Field(None, title='Run Duration')
    evaluation_cost: Optional[float] = Field(None, title='Evaluation Cost')


class TraceRunJudgmentType(BaseModel):
    project_name: Optional[str] = Field(None, title='Project Name')
    eval_name: Optional[str] = Field(None, title='Eval Name')
    traces: List[TraceJudgmentType] = Field(..., title='Traces')
    scorers: List[ScorerJudgmentType] = Field(..., title='Scorers')
    model: Union[str, List[str]] = Field(..., title='Model')
    aggregator: Optional[str] = Field(None, title='Aggregator')
    metadata: Optional[Dict[str, Any]] = Field({}, title='Metadata')
    judgment_api_key: Optional[str] = Field(None, title='Judgment Api Key')
    log_results: bool = Field(..., title='Log Results')
    append: Optional[bool] = Field(False, title='Append')
    override_existing_eval_run_name: Optional[bool] = Field(
        False, title='Override Existing Eval Run Name'
    )
    rules: Optional[List] = Field(None, title='Rules')
    trace_span_id: Optional[str] = Field(None, title='Trace Span Id')
    tools: Optional[List[Dict[str, Any]]] = Field(None, title='Tools')


class EvalResultsJudgmentType(BaseModel):
    results: List[ScoringResultJudgmentType] = Field(..., title='Results')
    run: Union[TraceRunJudgmentType, JudgmentEvalJudgmentType] = Field(..., title='Run')


class DatasetPushJudgmentType(BaseModel):
    dataset_alias: str = Field(..., title='Dataset Alias')
    comments: Optional[str] = Field(None, title='Comments')
    source_file: Optional[str] = Field(None, title='Source File')
    examples: Optional[List[ExampleJudgmentType]] = Field(None, title='Examples')
    traces: Optional[List[TraceJudgmentType]] = Field(None, title='Traces')
    is_trace: Optional[bool] = Field(False, title='Is Trace')
    project_name: str = Field(..., title='Project Name')
    overwrite: Optional[bool] = Field(False, title='Overwrite')
