---
title: Tracing
---

## Overview ##

`judgeval`'s tracing module allows you to view your LLM application's execution from **end-to-end**. 

Using tracing, you can:
- Gain observability into **every layer of your agentic system**, from database queries to tool calling and text generation.
- Measure the performance of **each system component in any way** you want to measure it. For instance:
    - Catch regressions in **retrieval quality, factuality, answer relevance**, and 10+ other [**research-backed metrics**](/evaluation/scorers/introduction).
    - Quantify the **quality of each tool call** your agent makes
    - Track the latency of each system component
    - Count the token usage of each LLM generation
- Export your workflow runs to the Judgment platform for **real-time analysis** or as a dataset for [**offline experimentation**](/evaluation/introduction).


## Tracing Your Workflow ##

Setting up tracing with `judgeval` takes two simple steps:

### 1. Initialize a tracer with your API keys and project name

<CodeGroup>
```Python Python
from judgeval.tracer import Tracer

# loads from JUDGMENT_API_KEY and JUDGMENT_ORG_ID env vars
judgment = Tracer(project_name="my_project")
```
```Typescript Typescript
import { Tracer } from 'judgeval';

// The getInstance method ensures a singleton Tracer is used
const judgment = Tracer.getInstance({ projectName: "my_project" });
```
</CodeGroup>

<Note>
    The [Judgment tracer](/api_reference/trace) is a singleton object that should be shared across your application. 
    Your project name will be used to organize your traces in one place on the Judgment platform.
</Note>


### 2. Wrap your workflow components

`judgeval` provides wrapping mechanisms for your workflow components:

#### `wrap()` ####
The `wrap()` function goes over your LLM client (e.g. OpenAI, Anthropic, etc.) and captures metadata surrounding your LLM calls, such as:
- Latency
- Token usage
- Prompt/Completion
- Model name

Here's an example of using `wrap()` on an OpenAI client:
<CodeGroup>
```Python Python
from openai import OpenAI
from judgeval.tracer import wrap

client = wrap(OpenAI())
```
```Typescript Typescript
import OpenAI from 'openai';
import { wrap } from 'judgeval';

const client = wrap(new OpenAI());
```
</CodeGroup>

#### `observe()` ####
The `observe()` decorator/function wraps any function or method to capture its execution as a span in your trace:
With deep tracing, you only need one `judgment.observe` at the top level, and all nested functions will be automatically traced.

<CodeGroup>
```Python Python
from judgeval.tracer import Tracer, wrap
from openai import OpenAI

judgment = Tracer(project_name="my_project", deep_tracing=True)  # Enable deep tracing
client = wrap(OpenAI())

# With deep tracing, you only need to observe the top-level function
@judgment.observe(span_type="function")
def my_workflow():
    # All function calls inside will be automatically traced
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": "Hello!"}]
    )
    return response.choices[0].message.content

result = my_workflow()  # This will trace the entire workflow
```
```Typescript Typescript
import { Tracer, wrap } from 'judgeval';
import OpenAI from 'openai';

const judgment = Tracer.getInstance({ 
    projectName: "my_project",
    deepTracing: true  # Enable deep tracing
});
const client = wrap(new OpenAI());

// With deep tracing, you only need to observe the top-level function
const myWorkflow = judgment.observe({ spanType: "function" })(async () => {
    # All function calls inside will be automatically traced
    const response = await client.chat.completions.create({
        model: "gpt-4o",
        messages: [{ role: "user", content: "Hello!" }],
    });
    return response.choices[0]?.message?.content;
});

myWorkflow().then(console.log);  # This will trace the entire workflow
```
</CodeGroup>

#### Putting it all Together
Here's a complete example of using judgeval's tracing mechanisms:
<CodeGroup>
```Python Python
from judgeval.tracer import Tracer, wrap
from openai import OpenAI

openai_client = wrap(OpenAI())
# loads from JUDGMENT_API_KEY and JUDGMENT_ORG_ID env vars
judgment = Tracer(project_name="my_project")

@judgment.observe(span_type="tool")
def my_tool():
    return "Hello world!"

@judgment.observe(span_type="function")
def my_llm_call():
    message = my_tool()
    res = openai_client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": message}]
    )
    return res.choices[0].message.content

# This implicitly starts a trace if one isn't active
# and saves it upon completion or error.
main_result = my_llm_call()
```
```Typescript Typescript
import { Tracer, wrap } from 'judgeval';
import OpenAI from 'openai';

const openaiClient = wrap(new OpenAI());
const judgment = Tracer.getInstance({ projectName: "my_project" });

async function myTool(): Promise<string> {
    return "Hello world!";
}
const observedMyTool = judgment.observe({ spanType: "tool" })(myTool);

async function myLlmCall(): Promise<string | null | undefined> {
    const message = await observedMyTool();
    const res = await openaiClient.chat.completions.create({
        model: "gpt-4o",
        messages: [{ role: "user", content: message }],
    });
    return res.choices[0]?.message?.content;
}
const observedLlmCall = judgment.observe({ spanType: "function" })(myLlmCall);

// Calling the observed function implicitly starts and saves the trace
async function runImplicitTrace() {
    try {
        const result = await observedLlmCall();
        console.log("Implicit trace completed:", result);
    } catch (error) {
        console.error("Implicit trace failed:", error);
        // Trace saved automatically by observe on completion/error
    }
}

runImplicitTrace();
```
</CodeGroup>

And the trace will appear on the Judgment platform as follows:

![Alt text](/images/basic_trace_example.png "Basic Trace Example")

### 3. Running Production Evaluations

Optionally, you can run asynchronous evaluations directly inside your traces.

This enables you to run evaluations on your **production data in real-time**, which can be useful for:
- **Guardrailing your production system** against quality regressions (hallucinations, toxic responses, revealing private data, etc.).
- Exporting production data for **offline experimentation** (e.g for A/B testing your workflow versions on relevant use cases).
- Getting **actionable insights** on how to fix common failure modes in your workflow (e.g. missing knowledge base info, suboptimal prompts, etc.). 

To execute an asynchronous evaluation, you can use the `trace.asyncEvaluate()` method (Typescript) or `judgment.async_evaluate()` (Python, assuming it operates on the currently active trace).

<CodeGroup>
```Python Python
from judgeval.tracer import Tracer
from judgeval.scorers import AnswerRelevancyScorer

judgment = Tracer(project_name="my_project", deep_tracing=True)

# With deep tracing, only the main function needs to be observed
@judgment.observe(span_type="function")
def main():
    query = "What is the capital of France?"
    # This function call will be automatically traced
    res = generate_response(query)
    
    # Assumes this runs within an active trace context implicitly started by @observe
    judgment.async_evaluate(
        scorers=[AnswerRelevancyScorer(threshold=1.0)],
        input=query,
        actual_output=res,
        model="gpt-4o",
    )
    return res

# This function will be automatically traced when called from main()
def generate_response(query):
    return "The capital of France is Paris."

main() # Call the observed function
```
```Typescript Typescript
import { Tracer, AnswerRelevancyScorer } from 'judgeval';

const judgment = Tracer.getInstance({ 
    projectName: "my_project",
    deepTracing: true
});

// With deep tracing, only the main function needs to be observed
const mainWithEval = judgment.observe({ spanType: "function" })(async () => {
    const query = "What is the capital of France?";
    # This function call will be automatically traced
    const res = generateResponse(query);

    const currentTrace = judgment.getCurrentTrace();
    if (currentTrace) {
        await currentTrace.asyncEvaluate(
            [new AnswerRelevancyScorer(1.0)], 
            {
                input: query,
                actualOutput: res,
                model: "gpt-4o"
            }
        );
    } else {
        console.warn("No active trace for async evaluation.");
    }
    return res;
});

// This function will be automatically traced when called from mainWithEval
function generateResponse(query) {
    return "The capital of France is Paris.";
}

// Explicitly start a trace for the evaluation
async function runMainWithEvalTrace() {
    const trace = judgment.startTrace("main_eval_trace");
    try {
        const result = await mainWithEval();
        console.log("Trace with eval completed:", result);
        await trace.save();
    } catch (error) {
        console.error("Trace with eval failed:", error);
        await trace.save();
    } finally {
        judgment.clearCurrentTrace(); // Clean up
    }
    return result;
}

runMainWithEvalTrace().then(console.log).catch(console.error);
```
</CodeGroup>

<Tip>
    Your async evaluations will be logged to the Judgment platform as part of the original trace and 
    a new evaluation will be created on the Judgment platform.
</Tip>

## Advanced: Deep Tracing and Agent-to-Agent Calls ##

Judgment's tracing system includes powerful deep tracing capabilities that automatically capture function calls within your application without requiring explicit `@observe` decorators on every function.

### Enabling Deep Tracing

Deep tracing can be enabled globally or for specific functions:

<CodeGroup>
```Python Python
# Enable deep tracing globally
judgment = Tracer(
    project_name="my_project",
    deep_tracing=True  # Enable for all observed functions
)

# Only the top-level function needs @observe
@judgment.observe(span_type="function")
def my_function():
    # All function calls inside will be traced automatically
    result = some_other_function()  # This will be traced without @observe
    return result

# No @observe needed - automatically traced when called from my_function
def some_other_function():
    return "Hello world!"
```
```Typescript Typescript
// Enable deep tracing globally
const judgment = Tracer.getInstance({ 
    projectName: "my_project",
    deepTracing: true  # Enable for all observed functions
});

// Only the top-level function needs observe
const myFunction = judgment.observe({ 
    spanType: "function"
})(async () => {
    # All function calls inside will be traced automatically
    const result = await someOtherFunction();  # This will be traced without observe
    return result;
});

// No observe needed - automatically traced when called from myFunction
async function someOtherFunction() {
    return "Hello world!";
}
```
</CodeGroup>

### Smart Function Filtering

Deep tracing intelligently filters out utility functions to keep your traces focused on meaningful application logic:

- **Excluded Functions**: Print statements, logging calls, and basic utility functions are automatically excluded
- **Module-Based Filtering**: Functions from standard modules like `logging`, `sys`, `os`, and others are not traced
- **Cleaner Visualization**: Your trace visualizations remain concise and focused on application-specific logic

### Tracing Agent-to-Agent (A2A) Calls

Judgment's tracing system can capture interactions between multiple agents in your system with minimal instrumentation:

<CodeGroup>
```Python Python
from judgeval.tracer import Tracer

judgment = Tracer(project_name="multi_agent_system", deep_tracing=True)

# Only need to observe the top-level agent
@judgment.observe(name="agent_1", span_type="agent")
def agent_1(query):
    # This agent processes the query
    processed = process_query(query)
    # Then calls agent_2 for additional processing
    return agent_2(processed)

# No @observe needed - automatically traced when called from agent_1
def agent_2(data):
    # Second agent processes data from agent_1
    return generate_response(data)

# No @observe needed - automatically traced when called from agent_2
def generate_response(data):
    return f"Response for: {data}"

# No @observe needed - automatically traced when called from agent_1
def process_query(query):
    return f"Processed: {query}"

# The trace will show the full flow from agent_1 to agent_2
# including all internal function calls thanks to deep tracing
with judgment.trace(name="multi_agent_workflow") as trace:
    result = agent_1("What's the weather in San Francisco?")
    trace.save()
```
```Typescript Typescript
import { Tracer } from 'judgeval';

const judgment = Tracer.getInstance({ 
    projectName: "multi_agent_system",
    deepTracing: true
});

// Only need to observe the top-level agent
const agent1 = judgment.observe({ 
    name: "agent_1", 
    spanType: "agent" 
})(async (query: string) => {
    # This agent processes the query
    const processed = processQuery(query);
    # Then calls agent_2 for additional processing
    return await agent2(processed);
});

// No observe needed - automatically traced when called from agent1
async function agent2(data: any) {
    # Second agent processes data from agent_1
    return generateResponse(data);
}

// No observe needed - automatically traced when called from agent2
function generateResponse(data: any) {
    return `Response for: ${data}`;
}

// No observe needed - automatically traced when called from agent1
function processQuery(query: string) {
    return `Processed: ${query}`;
}

// The trace will show the full flow from agent_1 to agent_2
async function runMultiAgentWorkflow() {
    const trace = judgment.startTrace("multi_agent_workflow");
    try {
        const result = await agent1("What's the weather in San Francisco?");
        console.log(result);
        await trace.save();
    } catch (error) {
        console.error(error);
        await trace.save();
    }
}
```
</CodeGroup>

This approach provides end-to-end visibility into complex multi-agent systems with minimal instrumentation, helping you understand how information flows between agents and identify bottlenecks or issues in your agent interactions.

## Example: Music Recommendation Agent

In this video, we'll walk through all of the topics covered in this guide by tracing over a simple OpenAI API-based music recommendation agent.

<iframe 
    width="560" 
    height="315" 
    src="https://www.youtube.com/embed/7g0fut06UxQ"
    title="YouTube video player" 
    frameborder="0" 
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" 
    referrerpolicy="strict-origin-when-cross-origin" 
    allowfullscreen
></iframe>

## Advanced: Customizing Traces Using the Context Manager (Python) / Explicit Trace Client (Typescript) ##

In Python, if you need to customize your tracing context beyond the implicit behavior of `@observe`, you can use the `with judgment.trace()` context manager.
In Typescript, you achieve similar control by explicitly creating a `TraceClient` instance using `judgment.startTrace()` and manually calling methods like `save()` or `print()` on it.

The explicit trace client allows you to **save or print the state of the trace at any point in the workflow**.
This is useful for debugging or exporting any state of your workflow to run an evaluation from!

<Tip>
    Any functions wrapped with `judgment.observe()` called within the scope where the `TraceClient` is active will automatically be associated with that trace.
</Tip>

Here's an example of using explicit trace management:
<CodeGroup>
```Python Python
from judgeval.tracer import Tracer, wrap
from openai import OpenAI

judgment = Tracer(project_name="my_project")
client = wrap(OpenAI())

@judgment.observe(span_type="tool")
def my_tool():
    return "Hello world!"

def main():
    with judgment.trace(name="my_workflow") as trace:
        res = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": f"{my_tool()}"}]
        )
    
        trace.print()  # prints the state of the trace to console
        trace.save()  # saves the current state of the trace to the Judgment platform
        # Note: Python trace context likely saves automatically on exit

    return res.choices[0].message.content
```
```Typescript Typescript
import { Tracer, wrap } from 'judgeval';
import OpenAI from 'openai';

const judgment = Tracer.getInstance({ projectName: "my_project" });
const client = wrap(new OpenAI());

async function myTool(): Promise<string> {
    return "Hello world!";
}
const observedMyTool = judgment.observe({ spanType: "tool" })(myTool);

async function main() {
    // Start the trace explicitly
    const trace = judgment.startTrace("my_workflow"); 
    let resultMessage: string | null | undefined;

    try {
        const toolOutput = await observedMyTool(); // observedMyTool is associated with the active trace
        const res = await client.chat.completions.create({
            model: "gpt-4o",
            messages: [{ role: "user", content: toolOutput }],
        });
        resultMessage = res.choices[0]?.message?.content;

        trace.print(); // Prints the trace to console
        await trace.save(); // Saves the trace to the Judgment platform
        
    } catch (error) {
        console.error("Workflow failed:", error);
        await trace.save(); // Still save trace on error
    } finally {
        judgment.clearCurrentTrace(); // Clean up
    }

    return resultMessage;
}

main().then(console.log).catch(console.error);
```
</CodeGroup>

<Warning>
    In Python, the `with judgment.trace()` context manager should only be used if you need fine-grained control 
    over the trace lifecycle. In Typescript, explicit management via `startTrace()` and `trace.save()` is the standard way to gain this control.
    In most simple cases in Python, the `@observe` decorator is sufficient.
</Warning>

## Integrations ##
{{ ... }}
