# generated by datamodel-codegen:
#   filename:  tmpmv2jt452.json
#   timestamp: 2026-01-20T05:12:26+00:00

from __future__ import annotations
from typing import Any, Dict, List, Literal, Optional, TypedDict, Union
from typing_extensions import NotRequired


class Example(TypedDict):
    example_id: str
    created_at: str
    name: NotRequired[Optional[str]]


DatasetKind = Literal["example", "trace"]


class ErrorResponse(TypedDict):
    error: str


class BaseScorer(TypedDict):
    score_type: str
    name: NotRequired[Optional[str]]
    class_name: NotRequired[Optional[str]]
    score: NotRequired[Optional[float]]
    minimum_score_range: NotRequired[float]
    maximum_score_range: NotRequired[float]
    score_breakdown: NotRequired[Optional[Dict[str, Any]]]
    reason: NotRequired[str]
    success: NotRequired[Optional[bool]]
    model: NotRequired[Optional[str]]
    error: NotRequired[Optional[str]]
    additional_metadata: NotRequired[Optional[Dict[str, Any]]]
    user: NotRequired[Optional[str]]
    server_hosted: bool
    using_native_model: NotRequired[Optional[bool]]
    required_params: NotRequired[Optional[List[str]]]
    strict_mode: NotRequired[Optional[bool]]


class ScorerConfig(TypedDict):
    score_type: str
    name: NotRequired[Optional[str]]
    threshold: float
    model: NotRequired[Optional[str]]
    required_params: NotRequired[Optional[List[str]]]
    kwargs: NotRequired[Optional[Dict[str, Any]]]


class ScorerData(TypedDict):
    id: Optional[str]
    name: str
    threshold: float
    success: bool
    score: NotRequired[float]
    minimum_score_range: NotRequired[Union[str, float]]
    maximum_score_range: NotRequired[Union[str, float]]
    reason: NotRequired[str]
    strict_mode: NotRequired[bool]
    evaluation_model: NotRequired[Optional[str]]
    error: NotRequired[Optional[str]]
    additional_metadata: NotRequired[Optional[Dict[str, Any]]]


class ScorersDatum(TypedDict):
    id: Optional[str]
    name: str
    threshold: float
    success: bool
    score: NotRequired[float]
    minimum_score_range: NotRequired[Union[str, float]]
    maximum_score_range: NotRequired[Union[str, float]]
    reason: NotRequired[str]
    strict_mode: NotRequired[bool]
    evaluation_model: NotRequired[Optional[str]]
    error: NotRequired[Optional[str]]
    additional_metadata: NotRequired[Optional[Dict[str, Any]]]


class DataObject(TypedDict):
    example_id: str
    created_at: str
    name: NotRequired[Optional[str]]


class ScoringResult1(TypedDict):
    success: bool
    scorers_data: List[ScorersDatum]
    name: NotRequired[Optional[str]]
    data_object: DataObject
    trace_id: NotRequired[Optional[str]]
    run_duration: NotRequired[Optional[float]]
    evaluation_cost: NotRequired[Optional[float]]


class Event(TypedDict):
    Timestamp: str
    Name: str
    Attributes: Dict[str, str]


class DataObject1(TypedDict):
    organization_id: str
    project_id: str
    user_id: str
    timestamp: str
    trace_id: str
    span_id: str
    parent_span_id: Optional[str]
    trace_state: Optional[str]
    span_name: Optional[str]
    span_kind: Optional[str]
    service_name: Optional[str]
    resource_attributes: Dict[str, Any]
    span_attributes: Dict[str, Any]
    duration: str
    status_code: float
    status_message: Optional[str]
    events: List[Event]
    links: Optional[str]


class ScoringResult2(TypedDict):
    success: bool
    scorers_data: List[ScorersDatum]
    name: NotRequired[str]
    data_object: DataObject1
    trace_id: NotRequired[str]
    run_duration: NotRequired[float]
    evaluation_cost: NotRequired[float]


ScoringResult = Union[ScoringResult1, ScoringResult2]


class CustomScorer(TypedDict):
    score_type: str
    name: NotRequired[Optional[str]]
    class_name: NotRequired[Optional[str]]
    score: NotRequired[Optional[float]]
    minimum_score_range: NotRequired[float]
    maximum_score_range: NotRequired[float]
    score_breakdown: NotRequired[Optional[Dict[str, Any]]]
    reason: NotRequired[str]
    success: NotRequired[Optional[bool]]
    model: NotRequired[Optional[str]]
    error: NotRequired[Optional[str]]
    additional_metadata: NotRequired[Optional[Dict[str, Any]]]
    user: NotRequired[Optional[str]]
    server_hosted: bool
    using_native_model: NotRequired[Optional[bool]]
    required_params: NotRequired[Optional[List[str]]]
    strict_mode: NotRequired[Optional[bool]]


class JudgmentScorer(TypedDict):
    score_type: str
    name: NotRequired[Optional[str]]
    threshold: float
    model: NotRequired[Optional[str]]
    required_params: NotRequired[Optional[List[str]]]
    kwargs: NotRequired[Optional[Dict[str, Any]]]


class EvaluationRun(TypedDict):
    id: NotRequired[Optional[str]]
    project_name: str
    eval_name: str
    model: NotRequired[Optional[str]]
    created_at: NotRequired[Optional[str]]
    user_id: NotRequired[str]
    scorers: NotRequired[Optional[List]]
    custom_scorers: List[CustomScorer]
    judgment_scorers: List[JudgmentScorer]


class ExampleEvaluationRun(TypedDict):
    id: NotRequired[Optional[str]]
    project_name: str
    eval_name: str
    model: NotRequired[Optional[str]]
    created_at: NotRequired[Optional[str]]
    user_id: NotRequired[str]
    scorers: NotRequired[Optional[List]]
    custom_scorers: List[CustomScorer]
    judgment_scorers: List[JudgmentScorer]
    examples: List[Example]
    trace_span_id: NotRequired[Optional[str]]
    trace_id: NotRequired[Optional[str]]


class ExampleEvaluationRunLegacy(TypedDict):
    id: NotRequired[Optional[str]]
    project_name: str
    eval_name: str
    model: NotRequired[Optional[str]]
    created_at: NotRequired[Optional[str]]
    user_id: NotRequired[str]
    scorers: NotRequired[Optional[List]]
    custom_scorers: NotRequired[List[CustomScorer]]
    judgment_scorers: NotRequired[List[JudgmentScorer]]
    examples: List[Example]
    trace_span_id: NotRequired[Optional[str]]
    trace_id: NotRequired[Optional[str]]


TraceAndSpanId = List[str]


class TraceEvaluationRun(TypedDict):
    id: NotRequired[Optional[str]]
    project_name: str
    eval_name: str
    model: NotRequired[Optional[str]]
    created_at: NotRequired[Optional[str]]
    user_id: NotRequired[str]
    scorers: NotRequired[Optional[List]]
    custom_scorers: List[CustomScorer]
    judgment_scorers: List[JudgmentScorer]
    trace_and_span_ids: List[TraceAndSpanId]
    is_offline: bool
    is_behavior: bool


class TraceEvaluationRunLegacy(TypedDict):
    id: NotRequired[Optional[str]]
    project_name: str
    eval_name: str
    model: NotRequired[Optional[str]]
    created_at: NotRequired[Optional[str]]
    user_id: NotRequired[str]
    scorers: NotRequired[Optional[List]]
    custom_scorers: NotRequired[List[CustomScorer]]
    judgment_scorers: NotRequired[List[JudgmentScorer]]
    trace_and_span_ids: List[TraceAndSpanId]
    is_offline: bool
    is_behavior: bool


class DataObject2(TypedDict):
    example_id: str
    created_at: str
    name: NotRequired[Optional[str]]


class ExampleScoringResult(TypedDict):
    success: bool
    scorers_data: List[ScorersDatum]
    name: NotRequired[Optional[str]]
    data_object: DataObject2
    trace_id: NotRequired[Optional[str]]
    run_duration: NotRequired[Optional[float]]
    evaluation_cost: NotRequired[Optional[float]]


class PromptCommitInfo(TypedDict):
    name: str
    prompt: str
    tags: List[str]
    commit_id: str
    parent_commit_id: Optional[str]
    created_at: str
    first_name: str
    last_name: str
    user_email: str


class PromptScorer(TypedDict):
    id: str
    user_id: str
    organization_id: str
    name: str
    prompt: str
    threshold: float
    model: str
    options: NotRequired[Optional[Dict[str, float]]]
    description: NotRequired[Optional[str]]
    created_at: Optional[str]
    updated_at: Optional[str]
    is_trace: NotRequired[bool]
    is_bucket_rubric: NotRequired[Optional[bool]]


class ExperimentScorer(TypedDict):
    scorer_data_id: str
    name: str
    score: float
    success: float
    reason: Optional[str]
    evaluation_model: Optional[str]
    threshold: float
    created_at: str
    error: Optional[str]


class Scorer(TypedDict):
    scorer_data_id: str
    name: str
    score: float
    success: float
    reason: Optional[str]
    evaluation_model: Optional[str]
    threshold: float
    created_at: str
    error: Optional[str]


class ExperimentRunItem(TypedDict):
    organization_id: str
    experiment_run_id: str
    example_id: str
    data: Any
    name: Optional[str]
    created_at: str
    scorers: List[Scorer]


class ProjectsResolvePostRequest(TypedDict):
    project_name: str


class ProjectsResolvePostResponse200(TypedDict):
    project_id: str


class ProjectsDeleteFromJudgevalDeleteResponse200(TypedDict):
    status: str
    message: str


class DatasetsCreateForJudgevalPostResponse200(TypedDict):
    detail: str


class DatasetsPullForJudgevalPostRequest(TypedDict):
    dataset_name: str
    project_name: str


class LogEvalResultsPostResponse200(TypedDict):
    ui_results_url: str


class FetchExperimentRunPostRequest(TypedDict):
    experiment_run_id: str
    project_name: str


class AddToRunEvalQueueExamplesPostResponse200(TypedDict):
    success: bool
    status: str
    message: str


class Commit(TypedDict):
    name: str
    prompt: str
    tags: List[str]
    commit_id: str
    parent_commit_id: Optional[str]
    created_at: str
    first_name: str
    last_name: str
    user_email: str


class PromptsFetchGetResponse200(TypedDict):
    commit: Optional[Commit]


class PromptsInsertPostRequest(TypedDict):
    project_id: str
    name: str
    prompt: str
    tags: NotRequired[List[str]]


class PromptsInsertPostResponse200(TypedDict):
    commit_id: str
    parent_commit_id: Optional[str]
    created_at: str


class PromptsTagPostRequest(TypedDict):
    project_id: str
    name: str
    commit_id: str
    tags: List[str]


class PromptsTagPostResponse200(TypedDict):
    commit_id: str


class PromptsUntagPostRequest(TypedDict):
    project_id: str
    name: str
    tags: List[str]


class PromptsUntagPostResponse200(TypedDict):
    commit_ids: List[str]


class FetchScorersPostRequest(TypedDict):
    names: NotRequired[Optional[List[str]]]
    is_trace: NotRequired[Optional[bool]]


class SaveScorerPostRequest(TypedDict):
    name: str
    prompt: str
    threshold: float
    model: str
    is_trace: bool
    options: NotRequired[Optional[Dict[str, float]]]
    description: NotRequired[Optional[str]]


class ScorerResponse(TypedDict):
    id: str
    user_id: str
    organization_id: str
    name: str
    prompt: str
    threshold: float
    model: str
    options: NotRequired[Optional[Dict[str, float]]]
    description: NotRequired[Optional[str]]
    created_at: Optional[str]
    updated_at: Optional[str]
    is_trace: NotRequired[bool]
    is_bucket_rubric: NotRequired[Optional[bool]]


class SaveScorerPostResponse200(TypedDict):
    scorer_response: ScorerResponse


class ScorerExistsPostRequest(TypedDict):
    name: str


class ScorerExistsPostResponse200(TypedDict):
    exists: bool


class UploadCustomScorerPostRequest(TypedDict):
    scorer_name: str
    scorer_code: str
    requirements_text: str
    class_name: str
    overwrite: bool
    scorer_type: NotRequired[Literal["example", "trace"]]


class UploadCustomScorerPostResponse200(TypedDict):
    scorer_name: str
    status: str
    message: str


class OtelTriggerRootSpanRulesPostRequestTracesItem(TypedDict):
    trace_id: str
    span_id: str


class DatasetsCreateForJudgevalPostRequestExamplesItem(TypedDict):
    example_id: str
    created_at: str
    name: NotRequired[Optional[str]]


class DatasetsPullAllForJudgevalPostResponse200Item(TypedDict):
    dataset_id: str
    name: str
    created_at: str
    kind: Literal["example", "trace"]
    entries: float
    creator: str


class LogEvalResultsPostRequestCustomScorersItem(TypedDict):
    score_type: str
    name: NotRequired[Optional[str]]
    class_name: NotRequired[Optional[str]]
    score: NotRequired[Optional[float]]
    minimum_score_range: NotRequired[float]
    maximum_score_range: NotRequired[float]
    score_breakdown: NotRequired[Optional[Dict[str, Any]]]
    reason: NotRequired[str]
    success: NotRequired[Optional[bool]]
    model: NotRequired[Optional[str]]
    error: NotRequired[Optional[str]]
    additional_metadata: NotRequired[Optional[Dict[str, Any]]]
    user: NotRequired[Optional[str]]
    server_hosted: bool
    using_native_model: NotRequired[Optional[bool]]
    required_params: NotRequired[Optional[List[str]]]
    strict_mode: NotRequired[Optional[bool]]


class LogEvalResultsPostRequestJudgmentScorersItem(TypedDict):
    score_type: str
    name: NotRequired[Optional[str]]
    threshold: float
    model: NotRequired[Optional[str]]
    required_params: NotRequired[Optional[List[str]]]
    kwargs: NotRequired[Optional[Dict[str, Any]]]


class PromptsGetPromptVersionsGetResponse200VersionsItem(TypedDict):
    name: str
    prompt: str
    tags: List[str]
    commit_id: str
    parent_commit_id: Optional[str]
    created_at: str
    first_name: str
    last_name: str
    user_email: str


class FetchScorersPostResponse200ScorersItem(TypedDict):
    id: str
    user_id: str
    organization_id: str
    name: str
    prompt: str
    threshold: float
    model: str
    options: NotRequired[Optional[Dict[str, float]]]
    description: NotRequired[Optional[str]]
    created_at: Optional[str]
    updated_at: Optional[str]
    is_trace: NotRequired[bool]
    is_bucket_rubric: NotRequired[Optional[bool]]


FetchExperimentRunPostResponse200AnyOfItemScorersItem = Any


LogEvalResultsPostRequestResultsItemScorersDataItem = Any


class OtelTriggerRootSpanRulesPostRequest(TypedDict):
    traces: List[OtelTriggerRootSpanRulesPostRequestTracesItem]


class DatasetsCreateForJudgevalPostRequest(TypedDict):
    name: str
    dataset_kind: Literal["example", "trace"]
    project_name: str
    examples: List[DatasetsCreateForJudgevalPostRequestExamplesItem]
    overwrite: bool


class DatasetsInsertExamplesForJudgevalPostRequest(TypedDict):
    dataset_name: str
    examples: List[DatasetsCreateForJudgevalPostRequestExamplesItem]
    project_name: str


class DatasetsPullForJudgevalPostResponse200(TypedDict):
    name: str
    project_name: str
    dataset_kind: Literal["example", "trace"]
    examples: List[DatasetsCreateForJudgevalPostRequestExamplesItem]


DatasetsPullAllForJudgevalPostResponse200 = List[
    DatasetsPullAllForJudgevalPostResponse200Item
]


class Run(TypedDict):
    id: NotRequired[Optional[str]]
    project_name: str
    eval_name: str
    model: NotRequired[Optional[str]]
    created_at: NotRequired[Optional[str]]
    user_id: NotRequired[str]
    scorers: NotRequired[Optional[List]]
    custom_scorers: List[LogEvalResultsPostRequestCustomScorersItem]
    judgment_scorers: List[LogEvalResultsPostRequestJudgmentScorersItem]
    examples: List[DatasetsCreateForJudgevalPostRequestExamplesItem]
    trace_span_id: NotRequired[Optional[str]]
    trace_id: NotRequired[Optional[str]]


class AddToRunEvalQueuePostRequest1(TypedDict):
    id: NotRequired[Optional[str]]
    project_name: str
    eval_name: str
    model: NotRequired[Optional[str]]
    created_at: NotRequired[Optional[str]]
    user_id: NotRequired[str]
    scorers: NotRequired[Optional[List]]
    custom_scorers: NotRequired[List[LogEvalResultsPostRequestCustomScorersItem]]
    judgment_scorers: NotRequired[List[LogEvalResultsPostRequestJudgmentScorersItem]]
    examples: List[DatasetsCreateForJudgevalPostRequestExamplesItem]
    trace_span_id: NotRequired[Optional[str]]
    trace_id: NotRequired[Optional[str]]


class AddToRunEvalQueuePostRequest2(TypedDict):
    id: NotRequired[Optional[str]]
    project_name: str
    eval_name: str
    model: NotRequired[Optional[str]]
    created_at: NotRequired[Optional[str]]
    user_id: NotRequired[str]
    scorers: NotRequired[Optional[List]]
    custom_scorers: NotRequired[List[LogEvalResultsPostRequestCustomScorersItem]]
    judgment_scorers: NotRequired[List[LogEvalResultsPostRequestJudgmentScorersItem]]
    trace_and_span_ids: List[TraceAndSpanId]
    is_offline: bool
    is_behavior: bool


AddToRunEvalQueuePostRequest = Union[
    AddToRunEvalQueuePostRequest1, AddToRunEvalQueuePostRequest2
]


class PromptsGetPromptVersionsGetResponse200(TypedDict):
    versions: List[PromptsGetPromptVersionsGetResponse200VersionsItem]


class FetchScorersPostResponse200(TypedDict):
    scorers: List[FetchScorersPostResponse200ScorersItem]


class LogEvalResultsPostRequestResultsItem(TypedDict):
    success: bool
    scorers_data: List[LogEvalResultsPostRequestResultsItemScorersDataItem]
    name: NotRequired[Optional[str]]
    data_object: DataObject2
    trace_id: NotRequired[Optional[str]]
    run_duration: NotRequired[Optional[float]]
    evaluation_cost: NotRequired[Optional[float]]


class FetchExperimentRunPostResponse200AnyOfItem(TypedDict):
    organization_id: str
    experiment_run_id: str
    example_id: str
    data: Any
    name: Optional[str]
    created_at: str
    scorers: List[FetchExperimentRunPostResponse200AnyOfItemScorersItem]


class LogEvalResultsPostRequest(TypedDict):
    results: List[LogEvalResultsPostRequestResultsItem]
    run: Run


class FetchExperimentRunPostResponse200(TypedDict):
    results: Optional[List[FetchExperimentRunPostResponse200AnyOfItem]]
    ui_results_url: Optional[str]
