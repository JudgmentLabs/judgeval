---
title: Datasets
---
## Overview
In most scenarios, you will have multiple `Example`s that you want to evaluate together.  
In `judgeval` (Python), an evaluation dataset (`EvalDataset`) is a collection of `Example`s that you can scale evaluations across. The Typescript library handles collections of examples as standard arrays passed to evaluation functions.

## Creating a Dataset (Python `EvalDataset`)

Creating an `EvalDataset` in Python is as simple as supplying a list of `Example`s.

```Python create_dataset.py
from judgeval.data import Example
from judgeval.data.datasets import EvalDataset

examples = [
    Example(input="Question 1?", actual_output="Answer 1."), 
    Example(input="Question 2?", actual_output="Answer 2."), 
    # ... more examples
]


dataset = EvalDataset(
    examples=examples
)
```

<Note>
In Typescript, you would typically manage your examples as an array:
`const examples: Example[] = [ example1, example2, ... ];`
</Note>

You can also add `Example`s to an existing `EvalDataset` in Python using the `add_example` method.

```Python add_to_dataset.py
from judgeval.data import Example
# Assume dataset = EvalDataset([...]) exists

dataset.add_example(Example(input="Question 3?", actual_output="Answer 3."))
```

<Note>
In Typescript, you would add to an array using standard methods like `push()`:
`examples.push(newExample);`
</Note>

## Saving/Loading Datasets (Python `EvalDataset`)

`judgeval` (Python) supports saving and loading `EvalDataset` objects in the following formats:
- JSON
- CSV
- YAML
- Judgment Platform

<Note>
These dataset management features (`push_dataset`, `pull_dataset`, `save_as`, `add_from_json`, etc.) are specific to the Python library's `EvalDataset` class.
</Note>

### From Judgment (Python)
```Python push_dataset.py
# Saving
from judgeval import JudgmentClient # Added import
from judgeval.data.datasets import EvalDataset # Added import
# Assume client = JudgmentClient() exists
# Assume dataset = EvalDataset(...) exists

client = JudgmentClient()
client.push_dataset(alias="my_dataset", dataset=dataset, project_name="my_project") # Added project_name based on TS client
```
```Python pull_dataset.py
# Loading
from judgeval import JudgmentClient # Added import
# Assume client = JudgmentClient() exists

client = JudgmentClient()
dataset = client.pull_dataset(alias="my_dataset", project_name="my_project") # Added project_name based on TS client
```

### From JSON (Python)

Your JSON file should have the following structure:
```json structure.json
{
    "examples": [
        {
            "input": "...", 
            "actual_output": "..."
        }, 
        ...
    ]
}
```

Here's an example of how use `judgeval` (Python) to save/load from JSON.

```Python json_dataset.py
from judgeval.data.datasets import EvalDataset

# saving
dataset = EvalDataset(...)  # filled with examples
dataset.save_as("json", "/path/to/save/dir", "save_name")

# loading
new_dataset = EvalDataset()
new_dataset.add_from_json("/path/to/your/json/file.json")

```

### From CSV (Python)

Your CSV should contain rows that can be mapped to `Example`s via column names.

```Python csv_dataset.py
from judgeval.data.datasets import EvalDataset

# saving
dataset = EvalDataset(...)  # filled with examples
dataset.save_as("csv", "/path/to/save/dir", "save_name")

# loading
new_dataset = EvalDataset()
new_dataset.add_from_csv("/path/to/your/csv/file.csv")
```

### From YAML (Python)

Your YAML should contain rows that can be mapped to `Example`s via column names.

```Python yaml_dataset.py
from judgeval.data.datasets import EvalDataset

# saving
dataset = EvalDataset(...)  # filled with examples
dataset.save_as("yaml", "/path/to/save/dir", "save_name")

# loading
new_dataset = EvalDataset()
new_dataset.add_from_yaml("/path/to/your/yaml/file.yaml")

```

```yaml example.yaml
examples:
  - input: ...
    actual_output: ...
    expected_output: ...
```

## Evaluate On Your Dataset / Examples

You can use the `JudgmentClient` to evaluate a collection of `Example`s using scorers.

<CodeGroup>
```Python evaluate_dataset.py
from judgeval import JudgmentClient # Added import
from judgeval.scorers import FaithfulnessScorer # Added import
# Assume client = JudgmentClient() exists
# Assume dataset = client.pull_dataset(alias="my_dataset", project_name="my_project") exists

res = client.evaluate_dataset(
    dataset=dataset, # Pass the EvalDataset object
    scorers=[FaithfulnessScorer(threshold=0.9)],
    model="gpt-4o",
)
```
```Typescript evaluate_dataset.ts
import { JudgmentClient, Example, FaithfulnessScorer, logger, ExampleBuilder } from 'judgeval';

// Assume client = JudgmentClient.getInstance() exists
const client = JudgmentClient.getInstance();

// Create an array of examples
const dataset: Example[] = [
    new ExampleBuilder().input("Q1").actualOutput("A1").build(),
    new ExampleBuilder().input("Q2").actualOutput("A2").build()
];

async function evaluateExamples() {
    const results = await client.evaluate({
        examples: dataset, // Pass the array of examples directly
        scorers: [new FaithfulnessScorer(0.9)],
        model: "gpt-4o",
        projectName: "dataset-eval-ts-proj",
        evalName: "dataset-eval-ts-run"
    });
    logger.print(results);
}

evaluateExamples();
```
</CodeGroup>

## Conclusion 

Congratulations! ðŸŽ‰

You've now learned how to create, save, and evaluate an `EvalDataset` in `judgeval`.

You can also view and manage your datasets via the [Judgment platform](/judgment/introduction).
