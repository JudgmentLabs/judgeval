# ðŸ¤– Judgment Labs â€“ EchoAgent Evaluation Project

This project demonstrates how to evaluate a simple AI agent using the open-source `judgeval` framework by Judgment Labs. It showcases the end-to-end pipeline from agent creation to scoring and accuracy measurement.

---

## ðŸ”§ Project Structure

- **`my_agents/agents/echo_agent.py`**  
  A minimal agent that echoes the input prompt with a prefix: _"You said: <prompt>"_.

- **`my_agents/agent_battle.py`**  
  Main evaluation script that:
  - Loads prompt-response examples from JSON
  - Calls the agent on each prompt
  - Scores outputs using a custom scorer (`AnswerRelevancyScorer`)
  - Prints results with pass/fail metrics

- **`my_agents/examples/prompts.json`**  
  Contains a list of test cases with expected inputs and outputs.

- **`src/judgeval/scorers/answer_relevancy_scorer.py`**  
  Custom scorer that compares agent responses with expected output using partial string matching logic.

---

## ðŸš€ How to Run the Evaluation

1. Set the Python path environment variable:

   ```
   export PYTHONPATH=src
   ```

2. Run the evaluation script:

   ```
   python my_agents/agent_battle.py
   ```

Youâ€™ll see the prompt, generated output, expected output, individual scores, and total accuracy.

---

## âœ… Example Output

```
Prompt: What is Python?
Generated: You said: What is Python?
Expected: Python is a high-level programming language.
Score: 0.17 | Passed: False
---
Prompt: Define variable.
Generated: You said: Define variable.
Expected: A variable stores a value in programming.
Score: 0.33 | Passed: False
---

âœ… Agent Accuracy: 0/2 passed (0.00%)
```

---

## ðŸ§  Why This Project

This mini-project was created as part of the **Judgment Labs AI Agent Battle**. It helps demonstrate:

- Agent evaluation using custom or built-in scorers
- Integration with the `judgeval` scoring system
- Ability to test and benchmark agents programmatically

---

## ðŸ™Œ Credits

Built by Sai Santoshi Praneetha Mukkamala forked from [Judgment Labs](https://github.com/JudgmentLabs/judgeval).
