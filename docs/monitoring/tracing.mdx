---
title: Tracing
---

## Overview ##

`judgeval`'s tracing module allows you to view your LLM application's execution from **end-to-end**. 

Using tracing, you can:
- Gain observability into **every layer of your agentic system**, from database queries to tool calling and text generation.
- Measure the performance of **each system component in any way** you want to measure it. For instance:
    - Catch regressions in **retrieval quality, factuality, answer relevance**, and 10+ other [**research-backed metrics**](/evaluation/scorers/introduction).
    - Quantify the **quality of each tool call** your agent makes
    - Track the latency of each system component
    - Count the token usage of each LLM generation
- Export your workflow runs to the Judgment platform for **real-time analysis** or as a dataset for [**offline experimentation**](/evaluation/introduction).


## Tracing Your Workflow ##

Setting up tracing with `judgeval` takes two simple steps:

### 1. Initialize a tracer with your API keys and project name

<CodeGroup>
```Python Python
from judgeval.common.tracer import Tracer

# loads from JUDGMENT_API_KEY and JUDGMENT_ORG_ID env vars
judgment = Tracer(project_name="my_project")
```
```Typescript Typescript
import { Tracer } from 'judgeval';

// The getInstance method ensures a singleton Tracer is used
const judgment = Tracer.getInstance({ projectName: "my_project" });
```
</CodeGroup>

<Note>
    The [Judgment tracer](/api_reference/trace) is a singleton object that should be shared across your application. 
    Your project name will be used to organize your traces in one place on the Judgment platform.
</Note>


### 2. Instrument your workflow components

`judgeval` provides mechanisms to instrument your workflow components:

#### `@observe` (Python) / `observe()` (Typescript) ####
The `@observe` decorator (Python) or the `observe()` higher-order function (Typescript) wraps your functions/tools and captures metadata surrounding your function calls, such as:
- Latency
- Input/Output
- Span type (e.g. `retriever`, `tool`, `LLM call`, etc.)

This is the primary way to define the spans (units of work) within your trace. Any instrumented LLM calls (using `wrap` or callbacks) made *inside* an observed function will be nested under that function's span.

Here's an example of using the observer mechanism:
<CodeGroup>
```Python Python
from judgeval.common.tracer import Tracer

# loads from JUDGMENT_API_KEY and JUDGMENT_ORG_ID env vars
judgment = Tracer(project_name="my_project")

@judgment.observe(span_type="tool")
def my_tool():
    print("Hello world!")

my_tool() # Calling the observed function starts/adds to a trace
```
```Typescript Typescript
import { Tracer } from 'judgeval';

const judgment = Tracer.getInstance({ projectName: "my_project" });

async function myTool(): Promise<void> {
    console.log("Hello world!");
}

const observedMyTool = judgment.observe({ spanType: "tool" })(myTool);

// You would then call observedMyTool() instead of myTool()
// Calling the observed function starts/adds to a trace
// await observedMyTool();
```
</CodeGroup>

<Note>
    `span_type` is a string that you can use to categorize and organize your trace spans. 
    Span types are displayed on the trace UI to easily navigate a visualization of your workflow. 
    Common span types include `tool`, `function`, `retriever`, `database`, `web search`, `llm`, etc.
</Note>

#### Instrumenting LLM Clients ####

To capture detailed information about LLM calls (latency, token usage, costs, prompts, completions) *within* your observed functions, you need to instrument the specific LLM client you are using. The approach varies slightly depending on the library.

##### Base SDKs (`openai`, `anthropic`, `togetherai`, `google-generativeai`) using `wrap()` #####

For direct SDK clients, the `wrap()` function automatically patches the client's methods to capture LLM call details.

<CodeGroup>
```Python Python
from openai import OpenAI
from judgeval.common.tracer import Tracer, wrap

# Initialize Tracer first
judgment = Tracer(project_name="my_project")

# Wrap the instantiated client
client = wrap(OpenAI())

@judgment.observe(span_type="function")
def simple_openai_call():
    res = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": "Hello!"}]
    )
    return res.choices[0].message.content

simple_openai_call()
```
```Typescript Typescript
import OpenAI from 'openai';
import { Tracer, wrap } from 'judgeval';

// Initialize Tracer first
const judgment = Tracer.getInstance({ projectName: "my_project" });

// Wrap the instantiated client
const client = wrap(new OpenAI());

async function simpleOpenaiCall(): Promise<string | null | undefined> {
    const res = await client.chat.completions.create({
        model: "gpt-4o",
        messages: [{ role: "user", content: "Hello!" }],
    });
    return res.choices[0]?.message?.content;
}
const observedSimpleCall = judgment.observe({ spanType: "function" })(simpleOpenaiCall);

// await observedSimpleCall();
```
</CodeGroup>

**Supported Clients for `wrap()`:**
*   `openai`: `OpenAI`, `AsyncOpenAI`, `AzureOpenAI`, `AsyncAzureOpenAI`
*   `anthropic`: `Anthropic`, `AsyncAnthropic`
*   `together`: `Together`, `AsyncTogether`
*   `google.generativeai`: `genai.Client`, `genai.client.AsyncClient` (for Gemini)

##### Langchain (`langchain-openai`, `langchain-anthropic`, etc.) using Callbacks #####

Langchain clients (like `ChatOpenAI`, `ChatAnthropic`) require using the `JudgmentTraceCallbackHandler`. This handler integrates with Langchain's callback system to capture LLM details.

<CodeGroup>
```Python Python
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage
from judgeval.common.tracer import Tracer, JudgmentTraceCallbackHandler

# Initialize Tracer first
judgment = Tracer(project_name="my_project")

# Instantiate the callback handler
judgment_callback = JudgmentTraceCallbackHandler()

# Instantiate Langchain client with the callback
client = ChatOpenAI(
    model="gpt-4o",
    callbacks=[judgment_callback] # Pass handler here
)

@judgment.observe(span_type="function")
def simple_langchain_call():
    res = client.invoke([HumanMessage(content="Hello!")])
    return res.content

simple_langchain_call()
```
```Typescript Typescript
import { ChatOpenAI } from "@langchain/openai"; // Assuming correct import path
import { HumanMessage } from "@langchain/core/messages";
import { Tracer, JudgmentTraceCallbackHandler } from 'judgeval'; // Assuming handler exists in TS SDK

// Initialize Tracer first
const judgment = Tracer.getInstance({ projectName: "my_project" });

// Instantiate the callback handler
const judgmentCallback = new JudgmentTraceCallbackHandler();

// Instantiate Langchain client with the callback
const client = new ChatOpenAI({
    model: "gpt-4o",
    callbacks: [judgmentCallback] // Pass handler here
});

async function simpleLangchainCall(): Promise<string | null | undefined> {
    const res = await client.invoke([new HumanMessage("Hello!")]);
    // Assuming 'content' is the correct property for the response type
    return res.content as string | undefined; 
}
const observedSimpleCall = judgment.observe({ spanType: "function" })(simpleLangchainCall);

// await observedSimpleCall();
```
</CodeGroup>
<Note>
   The `JudgmentTraceCallbackHandler` automatically creates nested LLM spans within the span created by `@judgment.observe()`. It correctly extracts token usage and other metadata required for cost calculation for supported Langchain chat models.
</Note>

### 3. Running Production Evaluations (Optional)

Optionally, you can run asynchronous evaluations directly inside your traces.

This enables you to run evaluations on your **production data in real-time**, which can be useful for:
- **Guardrailing your production system** against quality regressions (hallucinations, toxic responses, revealing private data, etc.).
- Exporting production data for **offline experimentation** (e.g for A/B testing your workflow versions on relevant use cases).
- Getting **actionable insights** on how to fix common failure modes in your workflow (e.g. missing knowledge base info, suboptimal prompts, etc.). 

To execute an asynchronous evaluation, you can use the `judgment.async_evaluate()` method (Python) or `trace.asyncEvaluate()` (Typescript, on an active trace client).

<CodeGroup>
```Python Python
from judgeval.common.tracer import Tracer
from judgeval.scorers import AnswerRelevancyScorer

judgment = Tracer(project_name="my_project")

@judgment.observe(span_type="function")
def main():
    query = "What is the capital of France?"
    res = "The capital of France is Paris."  # Replace with your workflow logic
    
    # Assumes this runs within an active trace context implicitly started by @observe
    judgment.async_evaluate(
        scorers=[AnswerRelevancyScorer(threshold=1.0)],
        input=query,
        actual_output=res,
        model="gpt-4o", # Specify model used for evaluation if applicable
    )
    return res

main() # Call the observed function
```
```Typescript Typescript
import { Tracer, AnswerRelevancyScorer } from 'judgeval';

const judgment = Tracer.getInstance({ projectName: "my_project" });

async function mainWithEval(): Promise<string> {
    const query = "What is the capital of France?";
    const res = "The capital of France is Paris."; // Replace with your workflow logic

    // Get the trace client associated with the current execution context
    // (This might be handled implicitly by observe or need explicit retrieval)
    const currentTrace = judgment.getCurrentTrace(); // Assumes such a method exists

    if (currentTrace) {
        // Call asyncEvaluate on the active trace client
        await currentTrace.asyncEvaluate(
            [new AnswerRelevancyScorer(1.0)], 
            {
                input: query,
                actualOutput: res,
                model: "gpt-4o" // Specify model used for evaluation if applicable
            }
        );
    } else {
        console.warn("No active trace for async evaluation. Ensure function is observed.");
    }
    return res;
}

// Observe the function. Calling it will run the evaluation within its trace.
const observedMainWithEval = judgment.observe({ spanType: "function" })(mainWithEval);

// await observedMainWithEval();
```
</CodeGroup>

<Tip>
    Your async evaluations will be logged to the Judgment platform as part of the original trace and 
    a new evaluation will be created on the Judgment platform.
</Tip>

## Example: Music Recommendation Agent

In this video, we'll walk through all of the topics covered in this guide by tracing over a simple OpenAI API-based music recommendation agent.

<iframe 
    width="560" 
    height="315" 
    src="https://www.youtube.com/embed/7g0fut06UxQ"
    title="YouTube video player" 
    frameborder="0" 
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" 
    referrerpolicy="strict-origin-when-cross-origin" 
    allowfullscreen
></iframe>


## Advanced: Customizing Traces Using the Context Manager (Python) / Explicit Trace Client (Typescript) ##

In Python, if you need to customize your tracing context beyond the implicit behavior of `@observe`, you can use the `with judgment.trace()` context manager.
In Typescript, you achieve similar control by explicitly creating a `TraceClient` instance using `judgment.startTrace()` and manually calling methods like `save()` or `print()` on it.

The explicit trace client allows you to **save or print the state of the trace at any point in the workflow**.
This is useful for debugging or exporting any state of your workflow to run an evaluation from!

<Tip>
    Any functions wrapped with `judgment.observe()` or instrumented clients (via `wrap` or callbacks) called within the scope where the `TraceClient` is active will automatically be associated with that trace.
</Tip>

Here's an example of using explicit trace management:
<CodeGroup>
```Python Python
from judgeval.common.tracer import Tracer, wrap
from openai import OpenAI

judgment = Tracer(project_name="my_project")
client = wrap(OpenAI()) # Using wrap for base SDK

@judgment.observe(span_type="tool")
def my_tool():
    return "Hello world!"

def main():
    # Start trace explicitly
    with judgment.trace(name="my_workflow") as trace:
        # my_tool() will be nested under this trace
        tool_output = my_tool() 
        # The client call will also be nested
        res = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": f"{tool_output}"}]
        )
    
        trace.print()  # prints the state of the trace to console
        # Note: Python trace context likely saves automatically on exit, 
        # but explicit save() can be used for intermediate states.
        # trace.save() 

        return res.choices[0].message.content

main()
```
```Typescript Typescript
import { Tracer, wrap } from 'judgeval';
import OpenAI from 'openai';

const judgment = Tracer.getInstance({ projectName: "my_project" });
const client = wrap(new OpenAI()); // Using wrap for base SDK

async function myTool(): Promise<string> {
    return "Hello world!";
}
// Observe the tool - it will attach to the active trace when called
const observedMyTool = judgment.observe({ spanType: "tool" })(myTool);

async function main() {
    // Start the trace explicitly
    const trace = judgment.startTrace("my_workflow"); 
    let resultMessage: string | null | undefined;

    try {
        // observedMyTool is associated with the active 'trace'
        const toolOutput = await observedMyTool(); 
        // The client call is also associated via wrap()
        const res = await client.chat.completions.create({
            model: "gpt-4o",
            messages: [{ role: "user", content: toolOutput }],
        });
        resultMessage = res.choices[0]?.message?.content;

        trace.print(); // Prints the trace to console
        await trace.save(); // Saves the trace to the Judgment platform
        
    } catch (error) {
        console.error("Workflow failed:", error);
        await trace.save(); // Still save trace on error
    }

    return resultMessage;
}

main();
```
</CodeGroup>

<Warning>
    In Python, the `with judgment.trace()` context manager should only be used if you need fine-grained control 
    over the trace lifecycle (e.g., multiple saves, access trace ID early). In Typescript, explicit management via `startTrace()` and `trace.save()` is the standard way to gain this control.
    In most simple cases in Python, the `@observe` decorator on your entry-point function is sufficient.
</Warning>







