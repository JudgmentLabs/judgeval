# ðŸ”Ž Judgeval: Custom Agent Scoring with AnswerRelevancyScorer

This fork of [JudgmentLabs/judgeval](https://github.com/JudgmentLabs/judgeval) contains my custom implementation to experiment with agent evaluation using a custom scorer and a simple echo-style agent.

## âœ… What I Added

### ðŸ§  Custom Scorer: `AnswerRelevancyScorer`
- Extended `JudgevalScorer` to create a new scorer called `AnswerRelevancyScorer`
- Performs string overlap scoring between the agent's output and the expected output
- Tracks score, threshold, and pass/fail status for each example

### ðŸ¤– Custom Agent: `EchoAgent`
- A basic agent that simply echoes the prompt input (used for testing and benchmarking)
- Located in `my_agents/agents/echo_agent.py`

### ðŸ§ª Evaluation Script: `agent_battle.py`
- Loads a list of prompt/expected_output pairs from JSON
- Runs the agent on each prompt
- Evaluates outputs using the custom scorer
- Prints score and whether the output passed the defined threshold

### ðŸ“‚ Prompts Example:
```json
[
  {
    "input": "What is Python?",
    "expected_output": "Python is a high-level programming language."
  },
  {
    "input": "Define variable.",
    "expected_output": "A variable stores a value in programming."
  }
]
